{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T00:36:57.792102Z",
     "start_time": "2020-04-18T00:36:57.782089Z"
    }
   },
   "source": [
    "# SFTP to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up SFTP server on **Windows_Server-2016-English-Full-Base**\n",
    "\n",
    "1. Add new Inbound Rules to Security Groups\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Port Range</th>\n",
    "    <th>Source</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20-21</td>\n",
    "    <td>Anywhere</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>Anywhere</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>50000-51000</td>\n",
    "    <td>Anywhere</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3369 & 3389</td>\n",
    "    <td>Anywhere</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "2. Download CopSSH and [Configure it](https://www.youtube.com/watch?v=aHKatBGrKbI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T14:01:52.424879Z",
     "start_time": "2020-04-18T14:01:52.406097Z"
    }
   },
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import math\n",
    "import boto3\n",
    "# Get the service client \n",
    "client = boto3.client('s3')\n",
    "\n",
    "import pprint\n",
    "# prints the formatted representation of PrettyPrinter object\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T00:33:50.485742Z",
     "start_time": "2020-04-18T00:33:50.482413Z"
    }
   },
   "source": [
    "## Connect to SFTP\n",
    "- We can create SFTPClient object connected to a computer on which remote file operations can be performed in two ways :-\n",
    "    1. Paramiko Transport object to establish a connection to the (remote) computer and then create the SFTClient object using the Transport object\n",
    "    2. Create a Paramiko SSHClient object which is then used to open a SFTP connection and obtain a SFTPClient object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:18.234901Z",
     "start_time": "2020-04-18T13:48:18.227909Z"
    }
   },
   "outputs": [],
   "source": [
    "host = \"ec2-15-206-125-2.ap-south-1.compute.amazonaws.com\"\n",
    "port = 22\n",
    "user = \"test_user\"\n",
    "pss = \"Rishabh@123\"\n",
    "keyfilepath = None\n",
    "bucket_name = \"rishabhsengar2611\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramiko Transport object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:19.222822Z",
     "start_time": "2020-04-18T13:48:18.239427Z"
    }
   },
   "outputs": [],
   "source": [
    "# If key is provided, then add the key\n",
    "if keyfilepath is not None:\n",
    "# Get private key used to authenticate user.\n",
    "    if keyfiletype == 'DSA':\n",
    "# The private key is a DSA type key.\n",
    "        key = paramiko.DSSKey.from_private_key_file(keyfilepath)\n",
    "    else:\n",
    "# The private key is a RSA type key.\n",
    "        key = paramiko.RSAKey.from_private_key(keyfilepath)\n",
    "\n",
    "    \n",
    "# Create Transport object using supplied method of authentication.\n",
    "transport = paramiko.Transport(host, port)\n",
    "# add key attribute if provided\n",
    "transport.connect( username = user, password = pss)\n",
    "sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "\n",
    "# # Close SFTP\n",
    "# sftp.close()\n",
    "# # Close tansport\n",
    "# transport.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramiko SSHClient object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:19.244467Z",
     "start_time": "2020-04-18T13:48:19.233780Z"
    }
   },
   "outputs": [],
   "source": [
    "# # If key is provided, then add the key\n",
    "# if keyfilepath is not None:\n",
    "# # Get private key used to authenticate user.\n",
    "#     if keyfiletype == 'DSA':\n",
    "# # The private key is a DSA type key.\n",
    "#         key = paramiko.DSSKey.from_private_key_file(keyfilepath)\n",
    "#     else:\n",
    "# # The private key is a RSA type key.\n",
    "#         key = paramiko.RSAKey.from_private_key(keyfilepath)\n",
    "\n",
    "# # Connect SSH client accepting all host keys.\n",
    "# ssh = paramiko.SSHClient()\n",
    "# ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "# # add key attribute if provided\n",
    "# ssh.connect(host, port, username = user, password = pss)\n",
    "\n",
    "# # Using the SSH client, create a SFTP client.\n",
    "# sftp = ssh.open_sftp()\n",
    "# # Keep a reference to the SSH client in the SFTP client as to prevent the former from\n",
    "# # being garbage collected and the connection from being closed.\n",
    "# sftp.sshclient = ssh\n",
    "\n",
    "# # Close SFTP\n",
    "# sftp.close()\n",
    "# # Close SSH\n",
    "# ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriving list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:19.700505Z",
     "start_time": "2020-04-18T13:48:19.253933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9G9VYW23CU.csv\n",
      "Big-Data-Landscape-2017.pdf\n",
      "Big-Data-Landscape-2018.pdf\n",
      "Big-Data-Landscape-2019.pdf\n",
      "D029LRLIRA.csv\n",
      "DN7A49XY69.csv\n",
      "EDTMQD3VFB.csv\n",
      "file.txt\n",
      "GJNZEL7QS7.csv\n",
      "Measurement_info.csv\n",
      "Measurement_summary.csv\n",
      "MOCK_DATA.json\n",
      "N84NQPAZ5A.csv\n",
      "people.json\n",
      "SG75B3AMDD.csv\n",
      "WMM6GGSTIQ.csv\n"
     ]
    }
   ],
   "source": [
    "# List files in the directory on the remote computer.\n",
    "dirlist = sftp.listdir('./ftp_files')\n",
    "for row in dirlist:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T01:47:15.046582Z",
     "start_time": "2020-04-18T01:47:15.037887Z"
    }
   },
   "source": [
    "### Get only CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:19.712074Z",
     "start_time": "2020-04-18T13:48:19.704078Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   '9G9VYW23CU.csv',\n",
      "    'D029LRLIRA.csv',\n",
      "    'DN7A49XY69.csv',\n",
      "    'EDTMQD3VFB.csv',\n",
      "    'GJNZEL7QS7.csv',\n",
      "    'Measurement_info.csv',\n",
      "    'Measurement_summary.csv',\n",
      "    'N84NQPAZ5A.csv',\n",
      "    'SG75B3AMDD.csv',\n",
      "    'WMM6GGSTIQ.csv']\n"
     ]
    }
   ],
   "source": [
    "def filter_names(n) :\n",
    "    if n.endswith('csv') :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "ftp_files = list(filter( filter_names, dirlist ))\n",
    "\n",
    "# Printing\n",
    "pp.pprint(ftp_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files which are already present in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.046215Z",
     "start_time": "2020-04-18T13:48:19.720820Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Contents': [   {   'ETag': '\"f903d3cf0c7f126fa9f939c3e07ee66d\"',\n",
      "                        'Key': 'EDTMQD3VFB.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 237,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a17ae887587039441d68f8d3ea1eeeb6\"',\n",
      "                        'Key': 'GJNZEL7QS7.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 316,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a111385963ba51827e8a407dcac1e867\"',\n",
      "                        'Key': 'MOCK_DATA.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 287,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"90782d0821d06338747f7927e53e3d00\"',\n",
      "                        'Key': 'SG75B3AMDD.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 387,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"fb05fa19d0981b560475a6753d3b3faa\"',\n",
      "                        'Key': 'WMM6GGSTIQ.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 277,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"d3ee934d3c9219a2f5a9230551cbf182\"',\n",
      "                        'Key': 'file.txt',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 319,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"9bcac1874c0a07eef4be75ea99e2272e\"',\n",
      "                        'Key': 'people.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 47, tzinfo=tzutc()),\n",
      "                        'Size': 1612897,\n",
      "                        'StorageClass': 'STANDARD'}],\n",
      "    'EncodingType': 'url',\n",
      "    'IsTruncated': False,\n",
      "    'KeyCount': 7,\n",
      "    'MaxKeys': 1000,\n",
      "    'Name': 'rishabhsengar2611',\n",
      "    'Prefix': '',\n",
      "    'ResponseMetadata': {   'HTTPHeaders': {   'content-type': 'application/xml',\n",
      "                                               'date': 'Sat, 18 Apr 2020 '\n",
      "                                                       '13:48:21 GMT',\n",
      "                                               'server': 'AmazonS3',\n",
      "                                               'transfer-encoding': 'chunked',\n",
      "                                               'x-amz-bucket-region': 'ap-south-1',\n",
      "                                               'x-amz-id-2': 'WF7bg2BlKO8uXPCHf9JgB6YevuICvn5+pVDlbLivpsA5spEpqWTdvL/b+NHf261KLugvmH76W0U=',\n",
      "                                               'x-amz-request-id': '7F7C5E1EDF7C7B09'},\n",
      "                            'HTTPStatusCode': 200,\n",
      "                            'HostId': 'WF7bg2BlKO8uXPCHf9JgB6YevuICvn5+pVDlbLivpsA5spEpqWTdvL/b+NHf261KLugvmH76W0U=',\n",
      "                            'RequestId': '7F7C5E1EDF7C7B09',\n",
      "                            'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    # Returns some or all of the objects in a bucket\n",
    "    s3_files = client.list_objects_v2(Bucket = 'rishabhsengar2611')\n",
    "except :\n",
    "    # if wrong bucket name is entered\n",
    "    print(\"No such Bucket \\n\")\n",
    "\n",
    "pp.pprint(s3_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.070115Z",
     "start_time": "2020-04-18T13:48:20.056098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EDTMQD3VFB.csv'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary Access to get the name of the first file\n",
    "s3_files['Contents'][0]['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.088960Z",
     "start_time": "2020-04-18T13:48:20.076887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary Access to get the size of the first file\n",
    "s3_files['Contents'][0]['Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.201849Z",
     "start_time": "2020-04-18T13:48:20.096553Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Contents': [   {   'ETag': '\"f903d3cf0c7f126fa9f939c3e07ee66d\"',\n",
      "                        'Key': 'EDTMQD3VFB.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 237,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a17ae887587039441d68f8d3ea1eeeb6\"',\n",
      "                        'Key': 'GJNZEL7QS7.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 316,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a111385963ba51827e8a407dcac1e867\"',\n",
      "                        'Key': 'MOCK_DATA.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 287,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"90782d0821d06338747f7927e53e3d00\"',\n",
      "                        'Key': 'SG75B3AMDD.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 387,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"fb05fa19d0981b560475a6753d3b3faa\"',\n",
      "                        'Key': 'WMM6GGSTIQ.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 277,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"d3ee934d3c9219a2f5a9230551cbf182\"',\n",
      "                        'Key': 'file.txt',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 319,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"9bcac1874c0a07eef4be75ea99e2272e\"',\n",
      "                        'Key': 'people.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 47, tzinfo=tzutc()),\n",
      "                        'Size': 1612897,\n",
      "                        'StorageClass': 'STANDARD'}],\n",
      "    'EncodingType': 'url',\n",
      "    'IsTruncated': False,\n",
      "    'KeyCount': 7,\n",
      "    'MaxKeys': 1000,\n",
      "    'Name': 'rishabhsengar2611',\n",
      "    'Prefix': '',\n",
      "    'ResponseMetadata': {   'HTTPHeaders': {   'content-type': 'application/xml',\n",
      "                                               'date': 'Sat, 18 Apr 2020 '\n",
      "                                                       '13:48:21 GMT',\n",
      "                                               'server': 'AmazonS3',\n",
      "                                               'transfer-encoding': 'chunked',\n",
      "                                               'x-amz-bucket-region': 'ap-south-1',\n",
      "                                               'x-amz-id-2': 'DAwXYMXhnlOCF+40TJKIUvQjhPQmiicUk02FTfZ8idg6V7q/b/YTc13Y+YtmxxCmMHmF160ES00=',\n",
      "                                               'x-amz-request-id': 'F36D4AA73A7A226C'},\n",
      "                            'HTTPStatusCode': 200,\n",
      "                            'HostId': 'DAwXYMXhnlOCF+40TJKIUvQjhPQmiicUk02FTfZ8idg6V7q/b/YTc13Y+YtmxxCmMHmF160ES00=',\n",
      "                            'RequestId': 'F36D4AA73A7A226C',\n",
      "                            'RetryAttempts': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Files which are already present in S3\n",
    "\n",
    "try :\n",
    "    # Returns some or all of the objects in a bucket\n",
    "    s3_files = client.list_objects_v2(Bucket = 'rishabhsengar2611')\n",
    "except :\n",
    "    # if wrong bucket name is entered\n",
    "    print(\"No such Bucket \\n\")\n",
    "\n",
    "pp.pprint(s3_files)\n",
    "\n",
    "# Dictionary Access to get the name of the first file\n",
    "s3_files['Contents'][0]['Key']\n",
    "\n",
    "# Dictionary Access to get the size of the first file\n",
    "s3_files['Contents'][0]['Size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S3 file names and there size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.219854Z",
     "start_time": "2020-04-18T13:48:20.203754Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name \t\t Size\n",
      "\n",
      "EDTMQD3VFB.csv\t\t237\n",
      "GJNZEL7QS7.csv\t\t316\n",
      "MOCK_DATA.json\t\t287\n",
      "SG75B3AMDD.csv\t\t387\n",
      "WMM6GGSTIQ.csv\t\t277\n",
      "file.txt\t\t319\n",
      "people.json\t\t1612897\n"
     ]
    }
   ],
   "source": [
    "# Used to store the s3 files names\n",
    "s3_files_names = []\n",
    "\n",
    "# Used to store the sizes of files\n",
    "s3_files_size = []\n",
    "\n",
    "print(\"File Name \\t\\t Size\\n\")\n",
    "# Print the files\n",
    "try :\n",
    "    for i in range( 0,len(s3_files['Contents']) ) :\n",
    "        s3_files_names.append(s3_files['Contents'][i]['Key'])\n",
    "        print(s3_files['Contents'][i]['Key'], end='\\t\\t')\n",
    "        s3_files_size.append(s3_files['Contents'][i]['Size'])\n",
    "        print(s3_files['Contents'][i]['Size'], end='\\n')\n",
    "        \n",
    "except KeyError :\n",
    "    print(\"Bucket is Empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove already present file in the s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.234247Z",
     "start_time": "2020-04-18T13:48:20.222168Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To be Added\n",
      "[   '9G9VYW23CU.csv',\n",
      "    'D029LRLIRA.csv',\n",
      "    'DN7A49XY69.csv',\n",
      "    'Measurement_info.csv',\n",
      "    'Measurement_summary.csv',\n",
      "    'N84NQPAZ5A.csv']\n",
      "\n",
      " Already Present\n",
      "['EDTMQD3VFB.csv', 'GJNZEL7QS7.csv', 'SG75B3AMDD.csv', 'WMM6GGSTIQ.csv']\n"
     ]
    }
   ],
   "source": [
    "# Already present files\n",
    "s3_files_present = []\n",
    "\n",
    "def filter_names(n) :\n",
    "    if n in s3_files_names :\n",
    "        s3_files_present.append(n)\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "ftp_files = list(filter( filter_names, ftp_files ))\n",
    "\n",
    "\n",
    "print(\" To be Added\")\n",
    "pp.pprint(ftp_files)\n",
    "print(\"\\n Already Present\")\n",
    "pp.pprint(s3_files_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.597893Z",
     "start_time": "2020-04-18T13:48:20.237179Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   '9G9VYW23CU.csv': 354,\n",
      "    'D029LRLIRA.csv': 341,\n",
      "    'DN7A49XY69.csv': 467,\n",
      "    'Measurement_info.csv': 124452984,\n",
      "    'Measurement_summary.csv': 94076158,\n",
      "    'N84NQPAZ5A.csv': 429}\n"
     ]
    }
   ],
   "source": [
    "# Make the Dictionary of filename and Size of the files present in FTP \n",
    "ftp_size = {}\n",
    "for i in ftp_files :\n",
    "# stat(path) ---> Retrieve information about a file on the remote system\n",
    "    destination = './ftp_files/' + i\n",
    "    info = sftp.stat(destination)\n",
    "    ftp_size[i] = info.st_size\n",
    "\n",
    "pp.pprint(ftp_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding files to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files which have size less than 6MB uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.639847Z",
     "start_time": "2020-04-18T13:48:20.622367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./ftp_files/9G9VYW23CU.csv'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftp_file_path = './ftp_files/' + list(ftp_size.keys())[0]\n",
    "print(ftp_file_path)\n",
    "\n",
    "ftp_file = sftp.file(ftp_file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:48:20.801763Z",
     "start_time": "2020-04-18T13:48:20.693479Z"
    }
   },
   "outputs": [],
   "source": [
    "ftp_file_data = ftp_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:49:53.397088Z",
     "start_time": "2020-04-18T13:49:52.813881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring complete File from FTP to S3...\n",
      "Successfully Transferred file from FTP to S3!\n"
     ]
    }
   ],
   "source": [
    "ftp_file = sftp.file(ftp_file_path, 'r')\n",
    "if list(ftp_size.values())[0] <= int(chunk_size): \n",
    "    #upload file in one go \n",
    "    print('Transferring complete File from FTP to S3...')\n",
    "#     ftp_file_data = ftp_file.read()\n",
    "    client.upload_fileobj(Fileobj=ftp_file ,Bucket = bucket_name ,Key = list(ftp_size.keys())[0]) \n",
    "    print('Successfully Transferred file from FTP to S3!') \n",
    "    ftp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T10:22:41.477673Z",
     "start_time": "2020-04-18T10:22:37.313Z"
    }
   },
   "source": [
    "### Files to be uploaded in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:55:33.593987Z",
     "start_time": "2020-04-18T13:55:33.585672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each part must be at least 5 MB in size\n",
    "# Since AWS won't allow us to have size less than 5MB\n",
    "# 1024*1024*6 == 6MB\n",
    "chunk_size = 6291456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:58:53.481313Z",
     "start_time": "2020-04-18T13:58:53.475670Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurement_info.csv\n",
      "Measurement_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Measurement_info.csv', 'Measurement_summary.csv']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Used to store the large files names\n",
    "big_files = []\n",
    "\n",
    "# This for loop is used to upload files\n",
    "for i in ftp_size :\n",
    "    if ftp_size[i] > chunk_size :\n",
    "        big_files.append(i)\n",
    "        \n",
    "# Big files are :-        \n",
    "big_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T14:01:30.431829Z",
     "start_time": "2020-04-18T14:01:30.418236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124452984"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftp_size[big_files[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T14:02:03.187161Z",
     "start_time": "2020-04-18T14:02:03.174286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_count = int(math.ceil(ftp_size[big_files[0]] / float(chunk_size)))\n",
    "chunk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T14:13:29.695070Z",
     "start_time": "2020-04-18T14:13:29.650516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ftp_files/Measurement_info.csv\n"
     ]
    }
   ],
   "source": [
    "ftp_file_path = './ftp_files/' + big_files[0]\n",
    "print(ftp_file_path)\n",
    "ftp_file = sftp.file(ftp_file_path, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T15:59:26.335676Z",
     "start_time": "2020-04-18T15:56:21.861622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ftp_files/Measurement_info.csv\n",
      "Transferring chunk 1...\n",
      "Chunk 1 Transferred Successfully!\n",
      "Transferring chunk 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Socket exception: Connection reset by peer (104)\n"
     ]
    },
    {
     "ename": "SSHException",
     "evalue": "Server connection dropped: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/paramiko/sftp_client.py\u001b[0m in \u001b[0;36m_read_response\u001b[0;34m(self, waitfor)\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/paramiko/sftp.py\u001b[0m in \u001b[0;36m_read_packet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0multra_debug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/paramiko/sftp.py\u001b[0m in \u001b[0;36m_read_all\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSSHException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-033bd10e7d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpart_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mftp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     part = client.upload_part(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/paramiko/file.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/paramiko/sftp_file.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         t, msg = self.sftp._request(\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mCMD_READ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCMD_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/paramiko/sftp_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, t, *arg)\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_async_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/paramiko/sftp_client.py\u001b[0m in \u001b[0;36m_read_response\u001b[0;34m(self, waitfor)\u001b[0m\n\u001b[1;32m    843\u001b[0m                 \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSHException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Server connection dropped: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m             \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSHException\u001b[0m: Server connection dropped: "
     ]
    }
   ],
   "source": [
    "multipart_upload = client.create_multipart_upload(Bucket = bucket_name, Key = big_files[0])\n",
    "parts = []\n",
    "\n",
    "ftp_file_path = './ftp_files/' + big_files[0]\n",
    "print(ftp_file_path)\n",
    "ftp_file = sftp.file(ftp_file_path, 'r')\n",
    "\n",
    "for  i in range(chunk_count):\n",
    "    print('Transferring chunk {}...'.format(i + 1))\n",
    "    part_number = i+1\n",
    "    \n",
    "    chunk = ftp_file.read(int(chunk_size))\n",
    "    \n",
    "    part = client.upload_part(\n",
    "        Bucket = bucket_name,\n",
    "        Key = big_files[0],\n",
    "        PartNumber = part_number,\n",
    "        UploadId = multipart_upload['UploadId'],\n",
    "        Body = chunk\n",
    "        )\n",
    "    \n",
    "    part_output = {'PartNumber': part_number,'ETag': part['ETag'] }\n",
    "                   \n",
    "    parts.append(part)\n",
    "    print('Chunk {} Transferred Successfully!'.format(i + 1))\n",
    "\n",
    "part_info = { 'Parts': parts }\n",
    "client.complete_multipart_upload(\n",
    "            Bucket = bucket_name,\n",
    "            Key = big_files[0],\n",
    "            UploadId = multipart_upload['UploadId'],\n",
    "            MultipartUpload = part_info\n",
    "            )\n",
    "print('All chunks Transferred to S3 bucket! File Transfer successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FTP to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T15:03:42.551024Z",
     "start_time": "2020-04-16T15:03:42.529909Z"
    }
   },
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pprint\n",
    "# prints the formatted representation of PrettyPrinter object\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "import boto3\n",
    "# Get the service client \n",
    "client = boto3.client('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting FTP Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T15:05:55.345576Z",
     "start_time": "2020-04-16T15:03:43.674601Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "[Errno 110] Connection timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-797f5e47eb31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Domain name or server ip:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mftp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFTP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ec2-52-66-211-38.ap-south-1.compute.amazonaws.com'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rishabh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ftplib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, host, user, passwd, acct, timeout, source_address)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ftplib.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, host, port, timeout, source_address)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         self.sock = socket.create_connection((self.host, self.port), self.timeout,\n\u001b[0;32m--> 152\u001b[0;31m                                              source_address=self.source_address)\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out"
     ]
    }
   ],
   "source": [
    "# from ftplib import FTP\n",
    "\n",
    "# Domain name or server ip:\n",
    "ftp = FTP('ec2-52-66-211-38.ap-south-1.compute.amazonaws.com')\n",
    "    \n",
    "ftp.login(user='test_user', passwd = 'rishabh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T14:13:22.962764Z",
     "start_time": "2020-04-16T14:11:11.883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the current directory on the server\n",
    "ftp.cwd('/files/ftp')\n",
    "\n",
    "# Return a list of file names\n",
    "ftp_files = ftp.nlst()\n",
    "\n",
    "# Printing\n",
    "pp.pprint(ftp_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get only CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.046925Z",
     "start_time": "2020-04-15T15:15:10.025481Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   '9G9VYW23CU.csv',\n",
      "    'D029LRLIRA.csv',\n",
      "    'DN7A49XY69.csv',\n",
      "    'EDTMQD3VFB.csv',\n",
      "    'GJNZEL7QS7.csv',\n",
      "    'Measurement_info.csv',\n",
      "    'Measurement_summary.csv',\n",
      "    'N84NQPAZ5A.csv',\n",
      "    'SG75B3AMDD.csv',\n",
      "    'WMM6GGSTIQ.csv']\n"
     ]
    }
   ],
   "source": [
    "def filter_names(n) :\n",
    "    if n.endswith('csv') :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "ftp_files = list(filter( filter_names, ftp_files ))\n",
    "\n",
    "# Printing\n",
    "pp.pprint(ftp_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files which are already present in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.509708Z",
     "start_time": "2020-04-15T15:15:10.051860Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Contents': [   {   'ETag': '\"a0ad38f3a09a0d0d0c2dd905736e6986\"',\n",
      "                        'Key': '9G9VYW23CU.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 13, tzinfo=tzutc()),\n",
      "                        'Size': 354,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"571d1690191e982850f6e593049ff428\"',\n",
      "                        'Key': 'D029LRLIRA.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 13, tzinfo=tzutc()),\n",
      "                        'Size': 341,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"8d2a79db85621494aa0c00583b0058cc\"',\n",
      "                        'Key': 'DN7A49XY69.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 467,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"f903d3cf0c7f126fa9f939c3e07ee66d\"',\n",
      "                        'Key': 'EDTMQD3VFB.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 237,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a17ae887587039441d68f8d3ea1eeeb6\"',\n",
      "                        'Key': 'GJNZEL7QS7.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 316,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a111385963ba51827e8a407dcac1e867\"',\n",
      "                        'Key': 'MOCK_DATA.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 287,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"473ceb460162431ab988c08eabd8577f\"',\n",
      "                        'Key': 'N84NQPAZ5A.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 429,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"90782d0821d06338747f7927e53e3d00\"',\n",
      "                        'Key': 'SG75B3AMDD.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 387,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"fb05fa19d0981b560475a6753d3b3faa\"',\n",
      "                        'Key': 'WMM6GGSTIQ.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 277,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"d3ee934d3c9219a2f5a9230551cbf182\"',\n",
      "                        'Key': 'file.txt',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 319,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"9bcac1874c0a07eef4be75ea99e2272e\"',\n",
      "                        'Key': 'people.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 47, tzinfo=tzutc()),\n",
      "                        'Size': 1612897,\n",
      "                        'StorageClass': 'STANDARD'}],\n",
      "    'EncodingType': 'url',\n",
      "    'IsTruncated': False,\n",
      "    'KeyCount': 11,\n",
      "    'MaxKeys': 1000,\n",
      "    'Name': 'rishabhsengar2611',\n",
      "    'Prefix': '',\n",
      "    'ResponseMetadata': {   'HTTPHeaders': {   'content-type': 'application/xml',\n",
      "                                               'date': 'Wed, 15 Apr 2020 '\n",
      "                                                       '15:15:11 GMT',\n",
      "                                               'server': 'AmazonS3',\n",
      "                                               'transfer-encoding': 'chunked',\n",
      "                                               'x-amz-bucket-region': 'ap-south-1',\n",
      "                                               'x-amz-id-2': 'BI8nFJsV/3fkfNgY4gOEo+qHYDrQdPgXEycDcDahgE7ArLLJsUk9wdWAH332Jum2eR30t/yoFK4=',\n",
      "                                               'x-amz-request-id': '7014ED663056D2BA'},\n",
      "                            'HTTPStatusCode': 200,\n",
      "                            'HostId': 'BI8nFJsV/3fkfNgY4gOEo+qHYDrQdPgXEycDcDahgE7ArLLJsUk9wdWAH332Jum2eR30t/yoFK4=',\n",
      "                            'RequestId': '7014ED663056D2BA',\n",
      "                            'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    # Returns some or all of the objects in a bucket\n",
    "    s3_files = client.list_objects_v2(Bucket = 'rishabhsengar2611')\n",
    "except :\n",
    "    # if wrong bucket name is entered\n",
    "    print(\"No such Bucket \\n\")\n",
    "\n",
    "pp.pprint(s3_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.521768Z",
     "start_time": "2020-04-15T15:15:10.514445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9G9VYW23CU.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary Access to get the name of the first file\n",
    "s3_files['Contents'][0]['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.535124Z",
     "start_time": "2020-04-15T15:15:10.526529Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary Access to get the size of the first file\n",
    "s3_files['Contents'][0]['Size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S3 file names and there size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.559219Z",
     "start_time": "2020-04-15T15:15:10.542839Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name \t\t Size\n",
      "\n",
      "9G9VYW23CU.csv\t\t354\n",
      "D029LRLIRA.csv\t\t341\n",
      "DN7A49XY69.csv\t\t467\n",
      "EDTMQD3VFB.csv\t\t237\n",
      "GJNZEL7QS7.csv\t\t316\n",
      "MOCK_DATA.json\t\t287\n",
      "N84NQPAZ5A.csv\t\t429\n",
      "SG75B3AMDD.csv\t\t387\n",
      "WMM6GGSTIQ.csv\t\t277\n",
      "file.txt\t\t319\n",
      "people.json\t\t1612897\n"
     ]
    }
   ],
   "source": [
    "# Used to store the s3 files names\n",
    "s3_files_names = []\n",
    "\n",
    "# Used to store the sizes of files\n",
    "s3_files_size = []\n",
    "\n",
    "print(\"File Name \\t\\t Size\\n\")\n",
    "# Print the files\n",
    "try :\n",
    "    for i in range( 0,len(s3_files['Contents']) ) :\n",
    "        s3_files_names.append(s3_files['Contents'][i]['Key'])\n",
    "        print(s3_files['Contents'][i]['Key'], end='\\t\\t')\n",
    "        s3_files_size.append(s3_files['Contents'][i]['Size'])\n",
    "        print(s3_files['Contents'][i]['Size'], end='\\n')\n",
    "        \n",
    "except KeyError :\n",
    "    print(\"Bucket is Empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove already present file in the s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.581135Z",
     "start_time": "2020-04-15T15:15:10.563067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To be Added\n",
      "['Measurement_info.csv', 'Measurement_summary.csv']\n",
      "\n",
      " Already Present\n",
      "[   '9G9VYW23CU.csv',\n",
      "    'D029LRLIRA.csv',\n",
      "    'DN7A49XY69.csv',\n",
      "    'EDTMQD3VFB.csv',\n",
      "    'GJNZEL7QS7.csv',\n",
      "    'N84NQPAZ5A.csv',\n",
      "    'SG75B3AMDD.csv',\n",
      "    'WMM6GGSTIQ.csv']\n"
     ]
    }
   ],
   "source": [
    "# Already present files\n",
    "s3_files_present = []\n",
    "\n",
    "def filter_names(n) :\n",
    "    if n in s3_files_names :\n",
    "        s3_files_present.append(n)\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "ftp_files = list(filter( filter_names, ftp_files ))\n",
    "\n",
    "\n",
    "print(\" To be Added\")\n",
    "pp.pprint(ftp_files)\n",
    "print(\"\\n Already Present\")\n",
    "pp.pprint(s3_files_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.683671Z",
     "start_time": "2020-04-15T15:15:10.584224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Measurement_info.csv': 124452984, 'Measurement_summary.csv': 94076158}\n"
     ]
    }
   ],
   "source": [
    "# Make the Dictionary of filename and Size of the files present in FTP \n",
    "ftp_size = {}\n",
    "for i in ftp_files :\n",
    "# FTP.size(filename) ---- >Request the size of the file named filename on the server.\n",
    "    ftp_size[i] = ftp.size(i)\n",
    "\n",
    "pp.pprint(ftp_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.689188Z",
     "start_time": "2020-04-15T15:15:10.685596Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each part must be at least 5 MB in size\n",
    "# Since AWS won't allow us to have size less than 5MB\n",
    "# 1024*1024*6 == 6MB\n",
    "chunk_size = 6291456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.697992Z",
     "start_time": "2020-04-15T15:15:10.691040Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurement_info.csv\t\t124452984\n",
      "Measurement_summary.csv\t\t94076158\n"
     ]
    }
   ],
   "source": [
    "# ftp.size(ftp_files[0])\n",
    "for file in ftp_size :\n",
    "    print(file+\"\\t\\t\"+str(ftp_size[file]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files which have size less than 6MB uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T15:15:10.706545Z",
     "start_time": "2020-04-15T15:15:10.699972Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big files :  Measurement_info.csv  -->  size :  124452984\n",
      "big files :  Measurement_summary.csv  -->  size :  94076158\n"
     ]
    }
   ],
   "source": [
    "# Used to store the large files\n",
    "big_files = []\n",
    "\n",
    "# This for loop is used to upload files\n",
    "for file in ftp_size :\n",
    "    if ftp_size[file] < chunk_size :\n",
    "        \n",
    "        # Change the location\n",
    "        local_file = os.path.join('/home/bluepi/Desktop/',file)\n",
    "        \n",
    "        # Retrieve a file in binary transfer mode\n",
    "        # RETR command --> A RETR request asks the server to send the contents of a file \n",
    "        #                       over the data connection already established by the client.\n",
    "        ftp.retrbinary('RETR ' + file, open(local_file, 'wb').write)\n",
    "        print(\"retieved file :\\t\" + file,end='\\n' )\n",
    "        \n",
    "        # Upload file in binary mode in s3 object\n",
    "        # Useful when we perform multipsrt upload\n",
    "        with open(local_file, 'rb') as data:\n",
    "            client.upload_fileobj(Fileobj=data, Bucket= 'rishabhsengar2611', Key= file)\n",
    "            \n",
    "        print(\"uploaded file :\\t\" + file,end='\\n')\n",
    "        #os.remove(file)\n",
    "    else :\n",
    "        big_files.append(file)\n",
    "        print(\"big files :  \"+file+\"  -->  size :  \"+str(ftp_size[file]),end = '\\n')\n",
    "        \n",
    "# Errors :-\n",
    "# Brokenpipeerror errno 32\n",
    "# ftplib.error_perm: 530 Login authentication failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FTP with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the list of files present in FTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T07:57:04.694044Z",
     "start_time": "2020-04-02T07:57:04.360332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 CWD successful. \"/files/ftp\" is current directory.\n",
      "['Big-Data-Landscape-2017.pdf', 'Big-Data-Landscape-2018.pdf', 'Big-Data-Landscape-2019.pdf', 'D029LRLIRA.csv', 'EDTMQD3VFB.csv', 'file.txt', 'MOCK_DATA.json', 'people.json', 'SG75B3AMDD.csv', 'WMM6GGSTIQ.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'221 Goodbye'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It allows us to write Python programs that perform a variety of automated FTP jobs\n",
    "\n",
    "# from ftplib import FTP_TLS\n",
    "# ftp = FTP_TLS()\n",
    "# ftp.debugging = 2\n",
    "# ftp.connect('192.168.1.7', 2121)\n",
    "# ftp.login('test_user', 'rishabh')\n",
    "\n",
    "# Return a new instance of the FTP class\n",
    "# connect to host, default port\n",
    "#ftp = ftplib.FTP_TLS(host=\"192.168.1.7\",user=\"test_user\",passwd=\"rishabh\")\n",
    "# Use \"FTP_TLS\" when FTP is over TLS\n",
    "\n",
    "\n",
    "from ftplib import FTP\n",
    "\n",
    "#domain name or server ip:\n",
    "ftp = FTP('13.233.66.93')\n",
    "ftp.login(user='test_user', passwd = 'rishabh')\n",
    "\n",
    "# Log in as the given user\n",
    "# Default user --> 'anonymous'\n",
    "# Default password --> 'anonymous@'\n",
    "# ftp.login(user=\"test_user\",passwd=\"rishabh\")\n",
    "\n",
    "# Set the current directory on the server\n",
    "print(ftp.cwd('/files/ftp'))\n",
    "\n",
    "# Return a list of file names\n",
    "lst = ftp.nlst()\n",
    "print(lst,end='\\n')  \n",
    "\n",
    "# Request the size of the file\n",
    "# f_size = ftp.\n",
    "# print( str(f_size) + ' Bytes' )\n",
    "\n",
    "# close the connection\n",
    "ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the file to be downloaded with this Spark job on every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T05:58:58.681233Z",
     "start_time": "2020-03-30T05:58:58.552870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Main entry point for Spark functionality\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# SparkFiles-- > Resolves paths to files added through \"SparkContext.addFile\"\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Get or instantiate a SparkContext and register it as a singleton object\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Basic structure of the FTP URL\n",
    "# ftp://<user>:<password>@<host>:<port>/<url-path>\n",
    "ftp_path = \"ftp://test_user:rishabh@192.168.1.7\"\n",
    "# filename = \"new_added.csv\"\n",
    "\n",
    "# Add a file to be downloaded with this Spark job on every node\n",
    "# \"lst\" --> contains the file path in ftp\n",
    "sc.addFile(ftp_path + lst[0])\n",
    "\n",
    "# Get the absolute path of a file\n",
    "absolute_path = SparkFiles.get(ftp_path + lst[1])\n",
    "\n",
    "# Get the root directory that contains files\n",
    "directory_path = SparkFiles.getRootDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the path where file is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T05:59:10.158256Z",
     "start_time": "2020-03-30T05:59:10.133637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/spark-a3d5a473-ef64-48b0-99ae-f48c45dc07f1/userFiles-bb12823e-d0df-4190-9703-819434301c81/main_table.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# import string\n",
    "# rpartition method returns a 3-tuple containing:\n",
    "#     *the part before the separator,\n",
    "#     *separator\n",
    "#     *part after the separator\n",
    "\n",
    "path2 = os.path.join(directory_path, lst[0].rpartition('/')[-1])\n",
    "# os.path.abspath(path2)\n",
    "path2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T05:59:17.350198Z",
     "start_time": "2020-03-30T05:59:13.438687Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()\n",
    "\n",
    "df = spark.read.format('csv') \\\n",
    "          .options( header=True, inferschema=False ) \\\n",
    "          .load(directory_path)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T06:12:37.336800Z",
     "start_time": "2020-03-30T06:12:37.125828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+\n",
      "|p_id|   p_name|price|\n",
      "+----+---------+-----+\n",
      "|   1|      Job| 1464|\n",
      "|   2|   Keylex|  208|\n",
      "|   3|   Duobam| 1684|\n",
      "|   4|Ronstring| 1961|\n",
      "|   5|  Bitwolf| 1338|\n",
      "|   6|  Andalax|   22|\n",
      "|   7|   Duobam| 1167|\n",
      "|   8|    Alpha| 1573|\n",
      "|   9|  Fix San| 1516|\n",
      "|  10|   Biodex| 1916|\n",
      "+----+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T05:59:29.202439Z",
     "start_time": "2020-03-30T05:59:28.929959Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# create the s3 instance \n",
    "client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T05:59:31.720098Z",
     "start_time": "2020-03-30T05:59:31.429563Z"
    }
   },
   "outputs": [],
   "source": [
    "response = client.put_object(\n",
    "    ACL='private',\n",
    "    Body=b'df', # Object data\n",
    "    Bucket='rishabhsengar2611', # Bucket name to which the PUT operation was initiated\n",
    "    Key='new_added.csv' # Object key for which the PUT operation was initiated\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "535px",
    "width": "464px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "301px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "432.5px",
    "left": "910px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
