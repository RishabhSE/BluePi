{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T00:36:57.792102Z",
     "start_time": "2020-04-18T00:36:57.782089Z"
    }
   },
   "source": [
    "# SFTP to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up SFTP server on **Windows_Server-2016-English-Full-Base**\n",
    "\n",
    "1. Add new Inbound Rules to Security Groups\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Port Range</th>\n",
    "    <th>Source</th>\n",
    "    <th>Why</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>20</td>\n",
    "    <td>Anywhere</td>\n",
    "      <th>For FTP(Data Channel)</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>21</td>\n",
    "    <td>Anywhere</td>\n",
    "      <th>For FTP(Command Channel)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>22</td>\n",
    "    <td>Anywhere</td>\n",
    "      <th>For SFTP</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>50000-51000</td>\n",
    "    <td>Anywhere</td>\n",
    "        <th>Passive Mode FTP</th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3369 & 3389</td>\n",
    "    <td>Anywhere</td>\n",
    "        <th>For RDP</th>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "2. Download CopSSH and [Configure it](https://www.youtube.com/watch?v=aHKatBGrKbI).\n",
    "\n",
    "\n",
    "**Active FTP Mode**\n",
    "- user connects from a random port on a file transfer client to FTP port 21 on the server\n",
    "- The server connects from port 20 to the client port designated for the data channel\n",
    "<img src=\"https://www.jscape.com/hs-fs/hub/26878/file-13611001-png/images/ftp_active_mode-resized-600.png\" />\n",
    "\n",
    "**Passive FTP Mode**\n",
    "- The client connects from a random port to port 21 on the server\n",
    "- The client connects from another random port to the random port specified in the server's response\n",
    "<img src=\"https://www.jscape.com/hs-fs/hub/26878/file-13611186-png/images/ftp_passive_mode-resized-600.png\" />\n",
    "\n",
    "**SFTP Mode**\n",
    "<img src=\" https://www.exavault.com/blog/app/uploads/2019/01/Screen-Shot-2019-01-14-at-4.30.46-PM-1024x544.png\" width = \"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:08:55.653570Z",
     "start_time": "2020-04-23T05:08:55.237345Z"
    }
   },
   "outputs": [],
   "source": [
    "import paramiko\n",
    "import math\n",
    "import boto3\n",
    "# Get the service client \n",
    "client = boto3.client('s3')\n",
    "\n",
    "import pprint\n",
    "# prints the formatted representation of PrettyPrinter object\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T00:33:50.485742Z",
     "start_time": "2020-04-18T00:33:50.482413Z"
    }
   },
   "source": [
    "## Connect to SFTP\n",
    "- We can create SFTPClient object connected to a computer on which remote file operations can be performed in two ways :-\n",
    "    1. Paramiko Transport object to establish a connection to the (remote) computer and then create the SFTClient object using the Transport object\n",
    "    2. Create a Paramiko SSHClient object which is then used to open a SFTP connection and obtain a SFTPClient object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:08:56.496101Z",
     "start_time": "2020-04-23T05:08:56.481571Z"
    }
   },
   "outputs": [],
   "source": [
    "host = \"ec2-13-127-30-239.ap-south-1.compute.amazonaws.com\"\n",
    "port = 22\n",
    "user = \"test_user\"\n",
    "pss = \"Rishabh@\"\n",
    "keyfilepath = None\n",
    "bucket_name = \"rishabhsengar2611\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramiko Transport object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:01.059913Z",
     "start_time": "2020-04-23T05:08:58.628552Z"
    }
   },
   "outputs": [],
   "source": [
    "# If key is provided, then add the key\n",
    "if keyfilepath is not None:\n",
    "# Get private key used to authenticate user.\n",
    "    if keyfiletype == 'DSA':\n",
    "# The private key is a DSA type key.\n",
    "        key = paramiko.DSSKey.from_private_key_file(keyfilepath)\n",
    "    else:\n",
    "# The private key is a RSA type key.\n",
    "        key = paramiko.RSAKey.from_private_key(keyfilepath)\n",
    "\n",
    "    \n",
    "# Create Transport object using supplied method of authentication.\n",
    "transport = paramiko.Transport(host, port)\n",
    "# add key attribute if provided\n",
    "transport.connect( username = user, password = pss)\n",
    "sftp = paramiko.SFTPClient.from_transport(transport)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:23.796507Z",
     "start_time": "2020-04-23T05:09:23.750712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transport.is_active()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramiko SSHClient object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T16:25:47.540341Z",
     "start_time": "2020-04-22T16:25:47.529735Z"
    }
   },
   "outputs": [],
   "source": [
    "# # If key is provided, then add the key\n",
    "# if keyfilepath is not None:\n",
    "# # Get private key used to authenticate user.\n",
    "#     if keyfiletype == 'DSA':\n",
    "# # The private key is a DSA type key.\n",
    "#         key = paramiko.DSSKey.from_private_key_file(keyfilepath)\n",
    "#     else:\n",
    "# # The private key is a RSA type key.\n",
    "#         key = paramiko.RSAKey.from_private_key(keyfilepath)\n",
    "\n",
    "# # Connect SSH client accepting all host keys.\n",
    "# ssh = paramiko.SSHClient()\n",
    "# ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "# # add key attribute if provided\n",
    "# ssh.connect(host, port, username = user, password = pss)\n",
    "\n",
    "# # Using the SSH client, create a SFTP client.\n",
    "# sftp = ssh.open_sftp()\n",
    "# # Keep a reference to the SSH client in the SFTP client as to prevent the former from\n",
    "# # being garbage collected and the connection from being closed.\n",
    "# sftp.sshclient = ssh\n",
    "\n",
    "# # Close SFTP\n",
    "# sftp.close()\n",
    "# # Close SSH\n",
    "# ssh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Manipulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriving list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:29.786001Z",
     "start_time": "2020-04-23T05:09:29.061922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "costs.csv\n",
      "date_1.csv\n",
      "Measurement_info.csv\n",
      "Measurement_summary.csv\n",
      "MOCK_DATA.json\n",
      "NoSQL.odt\n",
      "people.json\n",
      "WMM6GGSTIQ.csv\n"
     ]
    }
   ],
   "source": [
    "# List files in the directory on the remote computer.\n",
    "dirlist = sftp.listdir('./ftp_files')\n",
    "for row in dirlist:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T01:47:15.046582Z",
     "start_time": "2020-04-18T01:47:15.037887Z"
    }
   },
   "source": [
    "### Get only CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:37.643324Z",
     "start_time": "2020-04-23T05:09:37.624513Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   'costs.csv',\n",
      "    'date_1.csv',\n",
      "    'Measurement_info.csv',\n",
      "    'Measurement_summary.csv',\n",
      "    'WMM6GGSTIQ.csv']\n"
     ]
    }
   ],
   "source": [
    "def filter_names(n) :\n",
    "    if n.endswith('csv') :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "ftp_files = list(filter( filter_names, dirlist ))\n",
    "\n",
    "# Printing\n",
    "pp.pprint(ftp_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files which are already present in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:43.213768Z",
     "start_time": "2020-04-23T05:09:39.976430Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Contents': [   {   'ETag': '\"a0ad38f3a09a0d0d0c2dd905736e6986\"',\n",
      "                        'Key': '9G9VYW23CU.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 18, 13, 49, 54, tzinfo=tzutc()),\n",
      "                        'Size': 354,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"571d1690191e982850f6e593049ff428\"',\n",
      "                        'Key': 'D029LRLIRA.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 22, 11, 43, 19, tzinfo=tzutc()),\n",
      "                        'Size': 341,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"8d2a79db85621494aa0c00583b0058cc\"',\n",
      "                        'Key': 'DN7A49XY69.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 22, 12, 37, 31, tzinfo=tzutc()),\n",
      "                        'Size': 467,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"f903d3cf0c7f126fa9f939c3e07ee66d\"',\n",
      "                        'Key': 'EDTMQD3VFB.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 237,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a17ae887587039441d68f8d3ea1eeeb6\"',\n",
      "                        'Key': 'GJNZEL7QS7.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 316,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a111385963ba51827e8a407dcac1e867\"',\n",
      "                        'Key': 'MOCK_DATA.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 287,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"90782d0821d06338747f7927e53e3d00\"',\n",
      "                        'Key': 'SG75B3AMDD.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 387,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"fb05fa19d0981b560475a6753d3b3faa\"',\n",
      "                        'Key': 'WMM6GGSTIQ.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 277,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"d3ee934d3c9219a2f5a9230551cbf182\"',\n",
      "                        'Key': 'file.txt',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 319,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"9bcac1874c0a07eef4be75ea99e2272e\"',\n",
      "                        'Key': 'people.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 47, tzinfo=tzutc()),\n",
      "                        'Size': 1612897,\n",
      "                        'StorageClass': 'STANDARD'}],\n",
      "    'EncodingType': 'url',\n",
      "    'IsTruncated': False,\n",
      "    'KeyCount': 10,\n",
      "    'MaxKeys': 1000,\n",
      "    'Name': 'rishabhsengar2611',\n",
      "    'Prefix': '',\n",
      "    'ResponseMetadata': {   'HTTPHeaders': {   'content-type': 'application/xml',\n",
      "                                               'date': 'Thu, 23 Apr 2020 '\n",
      "                                                       '05:09:44 GMT',\n",
      "                                               'server': 'AmazonS3',\n",
      "                                               'transfer-encoding': 'chunked',\n",
      "                                               'x-amz-bucket-region': 'ap-south-1',\n",
      "                                               'x-amz-id-2': 'I3OY2DGqMx5Si3lpYw2KzASIixYPgO5KHS+UtnWm+biIS0j/mf026QZJqLPS36j7ME8sXFhzVdc=',\n",
      "                                               'x-amz-request-id': '9ABA82D5DD911DB7'},\n",
      "                            'HTTPStatusCode': 200,\n",
      "                            'HostId': 'I3OY2DGqMx5Si3lpYw2KzASIixYPgO5KHS+UtnWm+biIS0j/mf026QZJqLPS36j7ME8sXFhzVdc=',\n",
      "                            'RequestId': '9ABA82D5DD911DB7',\n",
      "                            'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    # Returns some or all of the objects in a bucket\n",
    "    s3_files = client.list_objects_v2(Bucket = 'rishabhsengar2611')\n",
    "except :\n",
    "    # if wrong bucket name is entered\n",
    "    print(\"No such Bucket \\n\")\n",
    "\n",
    "pp.pprint(s3_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:47.067015Z",
     "start_time": "2020-04-23T05:09:47.043664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9G9VYW23CU.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary Access to get the name of the first file\n",
    "s3_files['Contents'][0]['Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:48.476715Z",
     "start_time": "2020-04-23T05:09:48.463834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary Access to get the size of the first file\n",
    "s3_files['Contents'][0]['Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:09:58.903270Z",
     "start_time": "2020-04-23T05:09:58.670439Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Contents': [   {   'ETag': '\"a0ad38f3a09a0d0d0c2dd905736e6986\"',\n",
      "                        'Key': '9G9VYW23CU.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 18, 13, 49, 54, tzinfo=tzutc()),\n",
      "                        'Size': 354,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"571d1690191e982850f6e593049ff428\"',\n",
      "                        'Key': 'D029LRLIRA.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 22, 11, 43, 19, tzinfo=tzutc()),\n",
      "                        'Size': 341,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"8d2a79db85621494aa0c00583b0058cc\"',\n",
      "                        'Key': 'DN7A49XY69.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 22, 12, 37, 31, tzinfo=tzutc()),\n",
      "                        'Size': 467,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"f903d3cf0c7f126fa9f939c3e07ee66d\"',\n",
      "                        'Key': 'EDTMQD3VFB.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 237,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a17ae887587039441d68f8d3ea1eeeb6\"',\n",
      "                        'Key': 'GJNZEL7QS7.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 14, tzinfo=tzutc()),\n",
      "                        'Size': 316,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"a111385963ba51827e8a407dcac1e867\"',\n",
      "                        'Key': 'MOCK_DATA.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 287,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"90782d0821d06338747f7927e53e3d00\"',\n",
      "                        'Key': 'SG75B3AMDD.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 387,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"fb05fa19d0981b560475a6753d3b3faa\"',\n",
      "                        'Key': 'WMM6GGSTIQ.csv',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 49, 15, tzinfo=tzutc()),\n",
      "                        'Size': 277,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"d3ee934d3c9219a2f5a9230551cbf182\"',\n",
      "                        'Key': 'file.txt',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 45, tzinfo=tzutc()),\n",
      "                        'Size': 319,\n",
      "                        'StorageClass': 'STANDARD'},\n",
      "                    {   'ETag': '\"9bcac1874c0a07eef4be75ea99e2272e\"',\n",
      "                        'Key': 'people.json',\n",
      "                        'LastModified': datetime.datetime(2020, 4, 15, 11, 47, 47, tzinfo=tzutc()),\n",
      "                        'Size': 1612897,\n",
      "                        'StorageClass': 'STANDARD'}],\n",
      "    'EncodingType': 'url',\n",
      "    'IsTruncated': False,\n",
      "    'KeyCount': 10,\n",
      "    'MaxKeys': 1000,\n",
      "    'Name': 'rishabhsengar2611',\n",
      "    'Prefix': '',\n",
      "    'ResponseMetadata': {   'HTTPHeaders': {   'content-type': 'application/xml',\n",
      "                                               'date': 'Thu, 23 Apr 2020 '\n",
      "                                                       '05:09:59 GMT',\n",
      "                                               'server': 'AmazonS3',\n",
      "                                               'transfer-encoding': 'chunked',\n",
      "                                               'x-amz-bucket-region': 'ap-south-1',\n",
      "                                               'x-amz-id-2': 'LMEjByWpiemyjQymBP1WL7tuVcUJ6Xg1IHkGinzUC2iYYV8zJz5w6nqFIqHVf7zwZr6jqpHT1b4=',\n",
      "                                               'x-amz-request-id': 'D9A4AA690104562D'},\n",
      "                            'HTTPStatusCode': 200,\n",
      "                            'HostId': 'LMEjByWpiemyjQymBP1WL7tuVcUJ6Xg1IHkGinzUC2iYYV8zJz5w6nqFIqHVf7zwZr6jqpHT1b4=',\n",
      "                            'RequestId': 'D9A4AA690104562D',\n",
      "                            'RetryAttempts': 0}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Files which are already present in S3\n",
    "\n",
    "try :\n",
    "    # Returns some or all of the objects in a bucket\n",
    "    s3_files = client.list_objects_v2(Bucket = 'rishabhsengar2611')\n",
    "except :\n",
    "    # if wrong bucket name is entered\n",
    "    print(\"No such Bucket \\n\")\n",
    "\n",
    "pp.pprint(s3_files)\n",
    "\n",
    "# Dictionary Access to get the name of the first file\n",
    "s3_files['Contents'][0]['Key']\n",
    "\n",
    "# Dictionary Access to get the size of the first file\n",
    "s3_files['Contents'][0]['Size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S3 file names and there size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:10:02.332978Z",
     "start_time": "2020-04-23T05:10:02.300458Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name \t\t Size\n",
      "\n",
      "9G9VYW23CU.csv\t\t354\n",
      "D029LRLIRA.csv\t\t341\n",
      "DN7A49XY69.csv\t\t467\n",
      "EDTMQD3VFB.csv\t\t237\n",
      "GJNZEL7QS7.csv\t\t316\n",
      "MOCK_DATA.json\t\t287\n",
      "SG75B3AMDD.csv\t\t387\n",
      "WMM6GGSTIQ.csv\t\t277\n",
      "file.txt\t\t319\n",
      "people.json\t\t1612897\n"
     ]
    }
   ],
   "source": [
    "# Used to store the s3 files names\n",
    "s3_files_names = []\n",
    "\n",
    "# Used to store the sizes of files\n",
    "s3_files_size = []\n",
    "\n",
    "print(\"File Name \\t\\t Size\\n\")\n",
    "# Print the files\n",
    "try :\n",
    "    for i in range( 0,len(s3_files['Contents']) ) :\n",
    "        s3_files_names.append(s3_files['Contents'][i]['Key'])\n",
    "        print(s3_files['Contents'][i]['Key'], end='\\t\\t')\n",
    "        s3_files_size.append(s3_files['Contents'][i]['Size'])\n",
    "        print(s3_files['Contents'][i]['Size'], end='\\n')\n",
    "        \n",
    "except KeyError :\n",
    "    print(\"Bucket is Empty\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove already present file in the s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:10:07.818295Z",
     "start_time": "2020-04-23T05:10:07.798349Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To be Added\n",
      "['costs.csv', 'date_1.csv', 'Measurement_info.csv', 'Measurement_summary.csv']\n",
      "\n",
      " Already Present\n",
      "['WMM6GGSTIQ.csv']\n"
     ]
    }
   ],
   "source": [
    "# Already present files\n",
    "s3_files_present = []\n",
    "\n",
    "def filter_names(n) :\n",
    "    if n in s3_files_names :\n",
    "        s3_files_present.append(n)\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "ftp_files = list(filter( filter_names, ftp_files ))\n",
    "\n",
    "\n",
    "print(\" To be Added\")\n",
    "pp.pprint(ftp_files)\n",
    "print(\"\\n Already Present\")\n",
    "pp.pprint(s3_files_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:10:13.962313Z",
     "start_time": "2020-04-23T05:10:13.575710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Measurement_info.csv': 124452984,\n",
      "    'Measurement_summary.csv': 94076158,\n",
      "    'costs.csv': 612,\n",
      "    'date_1.csv': 749}\n"
     ]
    }
   ],
   "source": [
    "# Make the Dictionary of filename and Size of the files present in FTP \n",
    "ftp_size = {}\n",
    "for i in ftp_files :\n",
    "# stat(path) ---> Retrieve information about a file on the remote system\n",
    "    destination = './ftp_files/' + i\n",
    "    info = sftp.stat(destination)\n",
    "    ftp_size[i] = info.st_size\n",
    "\n",
    "pp.pprint(ftp_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding files to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files which have size less than 6MB uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:10:18.576735Z",
     "start_time": "2020-04-23T05:10:18.568943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each part must be at least 5 MB in size\n",
    "# Since AWS won't allow us to have size less than 5MB\n",
    "# 1024*1024*6 == 6MB\n",
    "chunk_size = 6291456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:13:35.834143Z",
     "start_time": "2020-04-23T05:13:35.757867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ftp_files/date_1.csv\n"
     ]
    }
   ],
   "source": [
    "ftp_file_path = './ftp_files/' + list(ftp_size.keys())[1]\n",
    "print(ftp_file_path)\n",
    "\n",
    "ftp_file = sftp.file(ftp_file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:13:36.741918Z",
     "start_time": "2020-04-23T05:13:36.556577Z"
    }
   },
   "outputs": [],
   "source": [
    "ftp_file_data = ftp_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:17:03.407336Z",
     "start_time": "2020-04-23T05:17:02.582858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferring complete File from FTP to S3...\n",
      "Successfully Transferred file from FTP to S3!\n"
     ]
    }
   ],
   "source": [
    "ftp_file = sftp.file(ftp_file_path, 'r')\n",
    "if list(ftp_size.values())[0] <= int(chunk_size): \n",
    "    #upload file in one go \n",
    "    print('Transferring complete File from FTP to S3...')\n",
    "#     ftp_file_data = ftp_file.read()\n",
    "    client.upload_fileobj(Fileobj=ftp_file ,Bucket = bucket_name ,Key = list(ftp_size.keys())[0]) \n",
    "    print('Successfully Transferred file from FTP to S3!') \n",
    "    ftp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T10:22:41.477673Z",
     "start_time": "2020-04-18T10:22:37.313Z"
    }
   },
   "source": [
    "### Files to be uploaded in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:57:42.403108Z",
     "start_time": "2020-04-23T05:57:42.386610Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Measurement_info.csv', 'Measurement_summary.csv']\n"
     ]
    }
   ],
   "source": [
    "# Used to store the large files names\n",
    "big_files = []\n",
    "\n",
    "# This for loop is used to upload files\n",
    "for i in ftp_size :\n",
    "    if ftp_size[i] > chunk_size :\n",
    "        big_files.append(i)\n",
    "        \n",
    "# Big files are :-        \n",
    "print(big_files)\n",
    "big_file = big_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:58:13.481109Z",
     "start_time": "2020-04-23T05:58:13.473482Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "size of Measurement_summary.csv is 94076158 bytes\n",
      "\n",
      "There are 15 chunks in which Measurement_summary.csv is broken down.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nsize of {} is {} bytes\".format(big_file,ftp_size[big_file]))\n",
    "\n",
    "chunk_count = int(math.ceil(ftp_size[big_file] / float(chunk_size)))\n",
    "print(\"\\nThere are {} chunks in which {} is broken down.\\n\".format(chunk_count,big_file))\n",
    "\n",
    "ftp_file_path = './ftp_files/' + big_file\n",
    "print(\"\\nThe Path from which file to be taken is :: {}\\n\".format(ftp_file_path))\n",
    "ftp_file = sftp.file(ftp_file_path, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Some Theory\n",
    "* There are few thing we should know before uploading files in chunks\n",
    "    - [create_multipart_upload()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.create_multipart_upload) \n",
    "        - this method initialtes the multipart upload opertaion and returns the Upload ID which is used to associate all the parts uploaded using this method. This Upload ID is specified in every part of the chunk.\n",
    "        \n",
    "             **NOTE** \n",
    "            - _After you initiate a multipart upload and upload one or more parts, to stop being charged for storing the uploaded parts, you must either complete or abort the multipart upload. Amazon S3 frees up the space used to store the parts and stop charging you for storing them only after you either complete or abort a multipart upload._\n",
    "            \n",
    "    - [abort_multipart_upload()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.abort_multipart_upload)\n",
    "        - This method is used to abort the multipart upload operation. After executing this method the storage consumed by any previously uploaded parts will be freed. \n",
    "        ```python\n",
    "            response = client.abort_multipart_upload(\n",
    "                        Bucket= bucket_name,\n",
    "                        Key=big_file,\n",
    "                        UploadId=multipart_upload_ID['UploadId'])\n",
    "            print(response)\n",
    "            \n",
    "        ```\n",
    "         **NOTE**\n",
    "        - _To verify that all parts have been removed, so you don't get charged for the part storage, you should call the ListParts operation and ensure that the parts list is empty._\n",
    "  \n",
    "    - [complete_multipart_upload()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.complete_multipart_upload)\n",
    "        - This method is used to assemble the previously uploaded parts with the given Upload ID.Upon receiving this request, Amazon S3 concatenates all the parts in ascending order by part number which is provided in part list along with is Entity Tag.\n",
    "        \n",
    "        **NOTE**\n",
    "        - Part List must be provided with the following structure\n",
    "        ```python\n",
    "           {\n",
    "            'Parts': \n",
    "               [   {\n",
    "                    'ETag': 'string',\n",
    "                    'PartNumber': 123\n",
    "               }   ]\n",
    "           }\n",
    "        ```\n",
    "    - [upload_part()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_part)\n",
    "\n",
    "        - This method is used to Upload a part one by one.\n",
    "        - _If you upload a new part using the same part number that was used with a previous part, the previously uploaded part is overwritten._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:40:47.161488Z",
     "start_time": "2020-04-23T05:20:35.734053Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ftp_files/Measurement_summary.csv\n",
      "Transferring chunk 1...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"c3e7bf42966af875af1ee617d77c5d3e\"', 'PartNumber': 1}\n",
      "Chunk 1 Transferred Successfully!\n",
      "Transferring chunk 2...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"cb52830a808fac50697eddf3cab88ca1\"', 'PartNumber': 2}\n",
      "Chunk 2 Transferred Successfully!\n",
      "Transferring chunk 3...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"60bfb964d720d6a92e3a125fbc1a765e\"', 'PartNumber': 3}\n",
      "Chunk 3 Transferred Successfully!\n",
      "Transferring chunk 4...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"c6d6223db9cd9850ea8a0447f3773e4d\"', 'PartNumber': 4}\n",
      "Chunk 4 Transferred Successfully!\n",
      "Transferring chunk 5...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"0ac135fc4d0cc8e6a3e28e8f3e936a6d\"', 'PartNumber': 5}\n",
      "Chunk 5 Transferred Successfully!\n",
      "Transferring chunk 6...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"e425c7cd789621e31b28b265ae24b5e3\"', 'PartNumber': 6}\n",
      "Chunk 6 Transferred Successfully!\n",
      "Transferring chunk 7...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"2b2571f9dd170dd2ec7cad27fb7e40a8\"', 'PartNumber': 7}\n",
      "Chunk 7 Transferred Successfully!\n",
      "Transferring chunk 8...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"e2de8ec79030a9cd60321f72069d35f9\"', 'PartNumber': 8}\n",
      "Chunk 8 Transferred Successfully!\n",
      "Transferring chunk 9...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"27813daa204e26d522861b5f67a870a8\"', 'PartNumber': 9}\n",
      "Chunk 9 Transferred Successfully!\n",
      "Transferring chunk 10...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"e577cb9212c8ffc957560999f0445e67\"', 'PartNumber': 10}\n",
      "Chunk 10 Transferred Successfully!\n",
      "Transferring chunk 11...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"ec16ada595c31745e4d7e5fc12615880\"', 'PartNumber': 11}\n",
      "Chunk 11 Transferred Successfully!\n",
      "Transferring chunk 12...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"e52938a021050abeb9a56144ade94e22\"', 'PartNumber': 12}\n",
      "Chunk 12 Transferred Successfully!\n",
      "Transferring chunk 13...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"095b87c4d964a585c07ed7b9613a1dbc\"', 'PartNumber': 13}\n",
      "Chunk 13 Transferred Successfully!\n",
      "Transferring chunk 14...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"19d161f8bde1cb6fa5af2b59444e93cd\"', 'PartNumber': 14}\n",
      "Chunk 14 Transferred Successfully!\n",
      "Transferring chunk 15...\n",
      "Connection not Dropped\n",
      "\n",
      "Reading the chunk\n",
      "\n",
      "Uploading the chunk\n",
      "\n",
      "{'ETag': '\"818e5e6930a62f77c4ca25ee0ee7cce0\"', 'PartNumber': 15}\n",
      "Chunk 15 Transferred Successfully!\n"
     ]
    }
   ],
   "source": [
    "# It initiates a multipart upload and returns an upload ID\n",
    "# upload ID is used to associate all of the parts in the specific multipart upload\n",
    "multipart_upload_ID = client.create_multipart_upload(Bucket = bucket_name, Key = big_file)\n",
    "\n",
    "parts = []\n",
    "\n",
    "# ftp_file_path--> the path from the root directory of the ftp server to the file( filename Included )\n",
    "ftp_file_path = './ftp_files/' + big_file\n",
    "print(ftp_file_path)\n",
    "\n",
    "# Open a file on the remote server\n",
    "# A file-like object is returned, which closely mimics the behavior of a normal Python file object\n",
    "ftp_file = sftp.file(ftp_file_path, 'r')\n",
    "\n",
    "\n",
    "for  i in range(chunk_count):\n",
    "    print('\\nTransferring chunk {}...'.format(i + 1))\n",
    "    part_number = i+1\n",
    "    \n",
    "    if transport.is_active() :\n",
    "        print(\"Connection not Dropped\\n\")\n",
    "    else :\n",
    "        print(\"Connection Dropped\\n\")\n",
    "    # Create Transport object using supplied method of authentication.\n",
    "        transport = paramiko.Transport(host, port)\n",
    "    # add key attribute if provided\n",
    "        transport.connect( username = user, password = pss)\n",
    "        sftp = paramiko.SFTPClient.from_transport(transport)\n",
    "        print(\"Connected again \\n\")\n",
    "\n",
    "    print(\"Reading the chunk\\n\")\n",
    "    \n",
    "    # Read up to length bytes from the given file\n",
    "    chunk = ftp_file.read(int(chunk_size))\n",
    "    \n",
    "    print(\"Uploading the chunk\\n\")\n",
    "    # It uploads a part in a multi-part upload\n",
    "    part = client.upload_part(\n",
    "        Bucket = bucket_name,\n",
    "        Key = big_file,\n",
    "        # PartNumber uniquely identifies a part and its position\n",
    "        PartNumber = part_number,\n",
    "        # Upload ID identifies the multipart_upload whose part is being uploaded.\n",
    "        UploadId = multipart_upload_ID['UploadId'],\n",
    "        Body = chunk\n",
    "        )\n",
    "    \n",
    "    # ETag ---> Entity tag for the uploaded object\n",
    "    part_output = {'PartNumber': part_number,'ETag': part['ETag'] }\n",
    "    pp.pprint(part_output)\n",
    "    \n",
    "    parts.append(part_output)\n",
    "    print('Chunk {} Transferred Successfully!'.format(i + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:43:20.548715Z",
     "start_time": "2020-04-23T05:43:20.260681Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All chunks Transferred to S3 bucket! File Transfer successful!\n"
     ]
    }
   ],
   "source": [
    "# In Complete_Multipart_Upload request,we must provide the parts list\n",
    "part_list = { 'Parts': parts }\n",
    "\n",
    "# It completes a multipart_upload by assembling previously uploaded parts.\n",
    "client.complete_multipart_upload(\n",
    "            Bucket = bucket_name,\n",
    "            Key = big_file,\n",
    "            UploadId = multipart_upload_ID['UploadId'],\n",
    "            MultipartUpload = part_list\n",
    "            )\n",
    "print('\\nAll chunks Transferred to S3 bucket! File Transfer successful!\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T08:36:19.443505Z",
     "start_time": "2020-04-23T08:36:19.436113Z"
    }
   },
   "outputs": [],
   "source": [
    "# Must be done\n",
    "# Close the file opened in the remote location\n",
    "ftp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T08:35:54.703236Z",
     "start_time": "2020-04-23T08:35:54.695208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transport.is_active()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-23T05:59:50.969529Z",
     "start_time": "2020-04-23T05:59:50.949860Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Parts': [   {   'ETag': '\"c3e7bf42966af875af1ee617d77c5d3e\"',\n",
      "                     'PartNumber': 1},\n",
      "                 {   'ETag': '\"cb52830a808fac50697eddf3cab88ca1\"',\n",
      "                     'PartNumber': 2},\n",
      "                 {   'ETag': '\"60bfb964d720d6a92e3a125fbc1a765e\"',\n",
      "                     'PartNumber': 3},\n",
      "                 {   'ETag': '\"c6d6223db9cd9850ea8a0447f3773e4d\"',\n",
      "                     'PartNumber': 4},\n",
      "                 {   'ETag': '\"0ac135fc4d0cc8e6a3e28e8f3e936a6d\"',\n",
      "                     'PartNumber': 5},\n",
      "                 {   'ETag': '\"e425c7cd789621e31b28b265ae24b5e3\"',\n",
      "                     'PartNumber': 6},\n",
      "                 {   'ETag': '\"2b2571f9dd170dd2ec7cad27fb7e40a8\"',\n",
      "                     'PartNumber': 7},\n",
      "                 {   'ETag': '\"e2de8ec79030a9cd60321f72069d35f9\"',\n",
      "                     'PartNumber': 8},\n",
      "                 {   'ETag': '\"27813daa204e26d522861b5f67a870a8\"',\n",
      "                     'PartNumber': 9},\n",
      "                 {   'ETag': '\"e577cb9212c8ffc957560999f0445e67\"',\n",
      "                     'PartNumber': 10},\n",
      "                 {   'ETag': '\"ec16ada595c31745e4d7e5fc12615880\"',\n",
      "                     'PartNumber': 11},\n",
      "                 {   'ETag': '\"e52938a021050abeb9a56144ade94e22\"',\n",
      "                     'PartNumber': 12},\n",
      "                 {   'ETag': '\"095b87c4d964a585c07ed7b9613a1dbc\"',\n",
      "                     'PartNumber': 13},\n",
      "                 {   'ETag': '\"19d161f8bde1cb6fa5af2b59444e93cd\"',\n",
      "                     'PartNumber': 14},\n",
      "                 {   'ETag': '\"818e5e6930a62f77c4ca25ee0ee7cce0\"',\n",
      "                     'PartNumber': 15}]}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(part_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close SFTP\n",
    "sftp.close()\n",
    "# Close tansport\n",
    "transport.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T05:05:44.248785Z",
     "start_time": "2020-04-24T05:05:44.228987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qqqqqq\n",
      "\n",
      "wwwww\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/bluepi/Documents/file.txt\", \"r\") as f:\n",
    "    print(f.read(6))\n",
    "    print(f.read(6))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
