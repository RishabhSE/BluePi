{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:45:57.241004Z",
     "start_time": "2020-03-25T17:45:57.229558Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Main entry point for DataFrame and SQL functionality.\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:45:57.814031Z",
     "start_time": "2020-03-25T17:45:57.790153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start SPARK Session\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:46:34.163649Z",
     "start_time": "2020-03-25T17:46:34.142327Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # import data\n",
    "# data = pd.read_csv(\"/home/bluepi/Downloads/Update/product_info/main table/product_info.csv\")\n",
    "\n",
    "# # convert day column type to datetime\n",
    "# data['day'] = pd.to_datetime( data['day'], infer_datetime_format=True, yearfirst=True)\n",
    "\n",
    "# # create new column\n",
    "# data['date_timestamp'] = pd.to_datetime(data.day.astype(str) + ' ' + data.time)\n",
    "\n",
    "# # Drop old columns\n",
    "# data.drop(['day','time'],inplace=True,axis=1)\n",
    "\n",
    "# # Write to csv\n",
    "# data.to_csv(\"/home/bluepi/Downloads/Update/Updated Product/Latest Product/main table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:43:29.614357Z",
     "start_time": "2020-03-25T17:43:29.163381Z"
    }
   },
   "outputs": [],
   "source": [
    "mainTable = spark.read.format('csv').options(\n",
    "    header=True, inferschema=True).load(\n",
    "        \"/home/bluepi/Downloads/Update/product_info/main table/main table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:43:29.807592Z",
     "start_time": "2020-03-25T17:43:29.802714Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainTable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:43:31.251915Z",
     "start_time": "2020-03-25T17:43:31.008522Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+----+------+------+\n",
      "|Year|Month|DayOfYear|Hour|Minute|Second|\n",
      "+----+-----+---------+----+------+------+\n",
      "|2020|    5|      123|   8|     6|    42|\n",
      "|2020|    2|       33|   8|     2|    22|\n",
      "|2020|    2|       59|   7|    58|     8|\n",
      "+----+-----+---------+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofyear, hour, minute, second\n",
    "\n",
    "mainTable.select([year(mainTable['date_timestamp']).alias(\"Year\"),\n",
    "                  month(mainTable['date_timestamp']).alias(\"Month\"),\n",
    "                  dayofyear(mainTable['date_timestamp']).alias(\"DayOfYear\"),\n",
    "                  hour(mainTable['date_timestamp']).alias(\"Hour\"),\n",
    "                  minute(mainTable['date_timestamp']).alias(\"Minute\"),\n",
    "                  second(mainTable['date_timestamp']).alias(\"Second\")]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:43:32.097130Z",
     "start_time": "2020-03-25T17:43:31.858881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|     date_timestamp|\n",
      "+-------------------+\n",
      "|2020-01-02 01:25:16|\n",
      "|2020-01-02 05:05:59|\n",
      "+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainTable.select('date_timestamp').orderBy(mainTable.date_timestamp.asc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:43:33.044435Z",
     "start_time": "2020-03-25T17:43:32.616523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Address to the product_info folder\n",
    "address = \"/home/bluepi/Downloads/Update/product_info/\"\n",
    "previous_day = (datetime.datetime.today() - datetime.timedelta(days=8)).strftime('%d-%m-%Y')\n",
    "\n",
    "# Address to the Previous Day folder\n",
    "new_address = address + previous_day\n",
    "\n",
    "# Read the Previous Day folder\n",
    "per_day_data = spark.read.format('csv') \\\n",
    "          .options( header=True, inferschema=True ) \\\n",
    "          .load(new_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:08:56.967558Z",
     "start_time": "2020-03-25T17:08:56.712547Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+-------------------+-----------+\n",
      "|p_id|     p_name|price|     Date_timestamp|record_type|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "| 248| Y-Solowarm| 1829|2020-03-17 00:47:21|          I|\n",
      "| 143|    Flexidy| 1312|2020-03-17 00:48:59|          U|\n",
      "|  32|     Latlux|  436|2020-03-17 01:01:39|          U|\n",
      "| 191|     Keylex| 1075|2020-03-17 01:02:32|          U|\n",
      "| 220|     Keylex| 1516|2020-03-17 01:05:44|          I|\n",
      "|  64|    Flexidy|  990|2020-03-17 01:21:40|          U|\n",
      "| 222|  Lotstring|  297|2020-03-17 01:27:48|          I|\n",
      "|  88|      Opela|  220|2020-03-17 01:28:35|          U|\n",
      "|  46|     Y-find|  325|2020-03-17 01:38:25|          U|\n",
      "| 236|   Aerified| 1955|2020-03-17 01:42:43|          I|\n",
      "| 209|   Wrapsafe| 1969|2020-03-17 01:49:04|          I|\n",
      "|  89|     Keylex| 1219|2020-03-17 02:02:57|          U|\n",
      "|  50|     Bamity|  287|2020-03-17 02:04:27|          U|\n",
      "|  96|   Overhold|  835|2020-03-17 02:06:11|          U|\n",
      "| 141|  Lotstring|  273|2020-03-17 06:48:26|          D|\n",
      "|   8|      Alpha| 1573|2020-03-17 06:48:40|          D|\n",
      "|  72|         It|  448|2020-03-17 07:00:20|          U|\n",
      "|  78|    Konklab| 1456|2020-03-17 07:00:33|          D|\n",
      "| 161| Voltsillam|  417|2020-03-17 07:14:04|          U|\n",
      "|  54|      Subin|  340|2020-03-17 07:26:46|          U|\n",
      "| 105|  Gembucket|  368|2020-03-17 07:36:27|          D|\n",
      "| 243|   Tampflex| 1341|2020-03-17 07:38:44|          I|\n",
      "|  47|   Transcof| 1465|2020-03-17 07:44:35|          U|\n",
      "| 137|     Tresom| 1439|2020-03-17 08:10:50|          U|\n",
      "| 232|     Latlux| 1316|2020-03-17 12:48:25|          I|\n",
      "| 206|      Alpha| 1182|2020-03-17 13:07:10|          I|\n",
      "| 189|     Lotlux|  180|2020-03-17 13:08:57|          U|\n",
      "| 159| Trippledex|  608|2020-03-17 13:10:25|          D|\n",
      "| 112|    Zontrax| 1056|2020-03-17 13:20:07|          U|\n",
      "|   4|  Ronstring| 1079|2020-03-17 13:27:36|          U|\n",
      "|  24|         It| 1447|2020-03-17 13:33:16|          U|\n",
      "| 210|Solarbreeze| 1630|2020-03-17 13:36:23|          I|\n",
      "| 250|   Alphazap| 1194|2020-03-17 13:40:02|          I|\n",
      "| 137|     Tresom|  420|2020-03-17 13:48:17|          U|\n",
      "|  18|     Lotlux|  565|2020-03-17 13:52:08|          D|\n",
      "|   3|     Duobam|  523|2020-03-17 13:53:52|          U|\n",
      "|  65|    Bitwolf| 1243|2020-03-17 14:08:08|          D|\n",
      "|  47|   Transcof|  300|2020-03-17 14:11:00|          U|\n",
      "| 227|    Fix San|  149|2020-03-17 18:59:22|          I|\n",
      "| 186|    Konklux|  826|2020-03-17 19:11:20|          U|\n",
      "| 126|    Sonsing|  311|2020-03-17 19:26:33|          U|\n",
      "| 131|  Daltfresh|   51|2020-03-17 19:28:37|          U|\n",
      "| 169|     Kanlam|  805|2020-03-17 19:48:56|          U|\n",
      "| 206|      Alpha| 1182|2020-03-17 20:08:34|          I|\n",
      "| 246|    Zontrax| 1107|2020-03-17 20:14:38|          I|\n",
      "| 204|        Job| 1047|2020-03-17 20:17:16|          I|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# per_day_data.show(3)\n",
    "per_day_data.orderBy(per_day_data.Date_timestamp.asc()).show(60)\n",
    "# per_day_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:43:49.143065Z",
     "start_time": "2020-03-25T17:43:49.132772Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create the schema\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# schema = StructType([StructField(\"p_id\", IntegerType(), True),\n",
    "#                      StructField(\"p_name\", StringType(), True),\n",
    "#                      StructField(\"price\", IntegerType(), True),\n",
    "#                      StructField(\"Date_timestamp\", TimestampType() , True),\n",
    "#                      StructField(\"record_type\", StringType(), True)\n",
    "#                      ])\n",
    "# # Create latest proct table\n",
    "# # Initially it is empty no updates\n",
    "# Latest_Product_Table = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:43:50.398133Z",
     "start_time": "2020-03-25T17:43:50.137235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+-------------------+-----------+\n",
      "|p_id|     p_name|price|     Date_timestamp|record_type|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "| 222|  Lotstring|  297|2020-03-17 01:27:48|          I|\n",
      "| 209|   Wrapsafe| 1969|2020-03-17 01:49:04|          I|\n",
      "| 220|     Keylex| 1516|2020-03-17 01:05:44|          I|\n",
      "| 248| Y-Solowarm| 1829|2020-03-17 00:47:21|          I|\n",
      "| 236|   Aerified| 1955|2020-03-17 01:42:43|          I|\n",
      "| 210|Solarbreeze| 1630|2020-03-17 13:36:23|          I|\n",
      "| 250|   Alphazap| 1194|2020-03-17 13:40:02|          I|\n",
      "| 232|     Latlux| 1316|2020-03-17 12:48:25|          I|\n",
      "| 206|      Alpha| 1182|2020-03-17 13:07:10|          I|\n",
      "| 243|   Tampflex| 1341|2020-03-17 07:38:44|          I|\n",
      "| 206|      Alpha| 1182|2020-03-17 20:08:34|          I|\n",
      "| 204|        Job| 1047|2020-03-17 20:17:16|          I|\n",
      "| 246|    Zontrax| 1107|2020-03-17 20:14:38|          I|\n",
      "| 227|    Fix San|  149|2020-03-17 18:59:22|          I|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_day_data.filter( per_day_data.record_type == 'I' ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T17:06:51.032039Z",
     "start_time": "2020-03-25T17:06:49.093841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT p_id)|\n",
      "+--------------------+\n",
      "|                 200|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct,count\n",
    "mainTable.select(countDistinct(mainTable.p_id)).show()\n",
    "# mainTable.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
