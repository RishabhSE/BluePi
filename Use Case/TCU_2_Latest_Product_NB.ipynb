{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T05:55:04.163537Z",
     "start_time": "2020-03-26T05:55:04.160257Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Main entry point for DataFrame and SQL functionality.\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T05:55:06.439860Z",
     "start_time": "2020-03-26T05:55:06.430947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start SPARK Session\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T18:59:17.037571Z",
     "start_time": "2020-03-25T18:59:17.033996Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # import data\n",
    "# data = pd.read_csv(\"/home/bluepi/Downloads/Update/product_info/main table/product_info.csv\")\n",
    "\n",
    "# # convert day column type to datetime\n",
    "# data['day'] = pd.to_datetime( data['day'], infer_datetime_format=True, yearfirst=True)\n",
    "\n",
    "# # create new column\n",
    "# data['date_timestamp'] = pd.to_datetime(data.day.astype(str) + ' ' + data.time)\n",
    "\n",
    "# # Drop old columns\n",
    "# data.drop(['day','time'],inplace=True,axis=1)\n",
    "\n",
    "# # Write to csv\n",
    "# data.to_csv(\"/home/bluepi/Downloads/Update/Updated Product/Latest Product/main table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T05:55:13.420142Z",
     "start_time": "2020-03-26T05:55:09.084263Z"
    }
   },
   "outputs": [],
   "source": [
    "mainTable = spark.read.format('csv').options(\n",
    "    header=True, inferschema=True).load(\n",
    "        \"/home/bluepi/Downloads/Update/product_info/main table/main table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T05:55:17.959605Z",
     "start_time": "2020-03-26T05:55:17.945731Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- p_id: integer (nullable = true)\n",
      " |-- p_name: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainTable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T05:55:23.287661Z",
     "start_time": "2020-03-26T05:55:22.919578Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---------+----+------+------+\n",
      "|Year|Month|DayOfYear|Hour|Minute|Second|\n",
      "+----+-----+---------+----+------+------+\n",
      "|2020|    5|      123|   8|     6|    42|\n",
      "|2020|    2|       33|   8|     2|    22|\n",
      "|2020|    2|       59|   7|    58|     8|\n",
      "+----+-----+---------+----+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofyear, hour, minute, second\n",
    "\n",
    "mainTable.select([year(mainTable['date_timestamp']).alias(\"Year\"),\n",
    "                  month(mainTable['date_timestamp']).alias(\"Month\"),\n",
    "                  dayofyear(mainTable['date_timestamp']).alias(\"DayOfYear\"),\n",
    "                  hour(mainTable['date_timestamp']).alias(\"Hour\"),\n",
    "                  minute(mainTable['date_timestamp']).alias(\"Minute\"),\n",
    "                  second(mainTable['date_timestamp']).alias(\"Second\")]).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T05:56:01.433999Z",
     "start_time": "2020-03-26T05:56:01.270456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|     date_timestamp|\n",
      "+-------------------+\n",
      "|2020-01-02 01:25:16|\n",
      "|2020-01-02 05:05:59|\n",
      "|2020-01-02 05:54:01|\n",
      "|2020-01-02 09:08:05|\n",
      "|2020-01-02 20:11:17|\n",
      "|2020-02-02 01:16:34|\n",
      "+-------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainTable.select('date_timestamp').orderBy(mainTable.date_timestamp.asc()).show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:20:55.443363Z",
     "start_time": "2020-03-26T09:20:55.148256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Address to the product_info folder\n",
    "address = \"/home/bluepi/Downloads/Update/product_info/\"\n",
    "previous_day = (datetime.datetime.today() - datetime.timedelta(days=1)).strftime('%d-%m-%Y')\n",
    "\n",
    "# Address to the Previous Day folder\n",
    "new_address = address + previous_day\n",
    "\n",
    "# Read the Previous Day folder\n",
    "per_day_data = spark.read.format('csv') \\\n",
    "          .options( header=True, inferschema=True ) \\\n",
    "          .load(new_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T09:20:56.372123Z",
     "start_time": "2020-03-26T09:20:56.074843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+-------------------+-----------+\n",
      "|p_id|     p_name|price|     Date_timestamp|record_type|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "| 242|   Greenlam| 1942|2020-03-24 17:05:17|          I|\n",
      "|  73|  Holdlamis| 1073|2020-03-24 17:09:01|          D|\n",
      "| 172|   Transcof|  827|2020-03-24 17:09:09|          U|\n",
      "| 100|Stringtough| 1240|2020-03-24 17:14:56|          U|\n",
      "|  40|   Treeflex|  247|2020-03-24 17:16:57|          D|\n",
      "|  66|Toughjoyfax|  517|2020-03-24 18:01:52|          U|\n",
      "| 234|    Bitchip|  377|2020-03-24 18:12:38|          I|\n",
      "| 164|      Alpha| 1167|2020-03-24 18:20:23|          U|\n",
      "| 229|     Zathin| 1408|2020-03-24 18:30:27|          I|\n",
      "| 185|    Konklab| 1460|2020-03-24 18:31:35|          U|\n",
      "| 238| Stronghold|  498|2020-03-24 23:02:41|          I|\n",
      "|  77|   Zaam-Dox|  336|2020-03-24 23:03:37|          U|\n",
      "| 118|    Flexidy| 1411|2020-03-24 23:10:05|          U|\n",
      "|  40|   Treeflex|  247|2020-03-24 23:14:43|          D|\n",
      "|  25|    Zontrax| 1533|2020-03-24 23:26:42|          D|\n",
      "|  28|     Latlux|  744|2020-03-24 23:28:39|          U|\n",
      "| 205|   Bytecard|  454|2020-03-24 23:37:32|          I|\n",
      "| 119|       Rank|  972|2020-03-24 23:47:53|          D|\n",
      "|  32|     Latlux|  580|2020-03-24 23:51:54|          U|\n",
      "| 224|   Wrapsafe|  935|2020-03-25 00:04:26|          I|\n",
      "| 148|Ventosanzap|  385|2020-03-25 00:08:00|          U|\n",
      "| 165|    Fixflex| 1191|2020-03-25 00:27:50|          U|\n",
      "| 201|   Flowdesk| 1267|2020-03-25 00:31:10|          I|\n",
      "| 194|     Sonair| 1070|2020-03-25 05:21:02|          U|\n",
      "| 216|     Lotlux|  613|2020-03-25 05:49:42|          I|\n",
      "| 244|  Daltfresh|  169|2020-03-25 05:58:30|          I|\n",
      "|  35|     Lotlux|  162|2020-03-25 06:00:55|          U|\n",
      "| 221|   Wrapsafe|  454|2020-03-25 06:07:16|          I|\n",
      "|  85|     Zoolab|  872|2020-03-25 06:09:02|          U|\n",
      "|  76|     Bigtax| 1472|2020-03-25 06:17:08|          D|\n",
      "| 207|   Alphazap| 1021|2020-03-25 06:26:46|          I|\n",
      "|  77|   Zaam-Dox|  811|2020-03-25 11:04:37|          U|\n",
      "|  46|     Y-find| 1341|2020-03-25 11:06:06|          U|\n",
      "| 142|     Biodex|  480|2020-03-25 11:19:13|          U|\n",
      "|  81|    Cardify| 1442|2020-03-25 11:19:37|          U|\n",
      "| 148|Ventosanzap| 1470|2020-03-25 11:26:56|          U|\n",
      "| 175|    Flexidy| 1125|2020-03-25 11:34:43|          U|\n",
      "|   6|    Andalax|   22|2020-03-25 11:46:22|          D|\n",
      "| 168|    Bitchip| 1231|2020-03-25 12:00:21|          D|\n",
      "| 188|      Alpha| 1201|2020-03-25 12:04:58|          U|\n",
      "| 168|    Bitchip|  581|2020-03-25 12:07:32|          U|\n",
      "+----+-----------+-----+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# per_day_data.show(3)\n",
    "per_day_data.orderBy(per_day_data.Date_timestamp.asc()).show(60)\n",
    "# per_day_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T18:59:23.776675Z",
     "start_time": "2020-03-25T18:59:23.766567Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Create the schema\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# schema = StructType([StructField(\"p_id\", IntegerType(), True),\n",
    "#                      StructField(\"p_name\", StringType(), True),\n",
    "#                      StructField(\"price\", IntegerType(), True),\n",
    "#                      StructField(\"Date_timestamp\", TimestampType() , True),\n",
    "#                      StructField(\"record_type\", StringType(), True)\n",
    "#                      ])\n",
    "# # Create latest proct table\n",
    "# # Initially it is empty no updates\n",
    "# Latest_Product_Table = spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T08:52:05.521448Z",
     "start_time": "2020-03-26T08:52:05.233267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----+-------------------+\n",
      "|p_id|     date_timestamp|p_id|     date_timestamp|\n",
      "+----+-------------------+----+-------------------+\n",
      "| 250|2020-03-20 19:42:54| 250|2020-03-20 19:42:54|\n",
      "| 228|2020-03-20 20:03:41| 228|2020-03-20 13:28:37|\n",
      "| 228|2020-03-20 20:03:41| 228|2020-03-20 20:03:41|\n",
      "| 243|2020-03-20 19:36:02| 243|2020-03-20 19:36:02|\n",
      "| 237|2020-03-20 20:00:52| 237|2020-03-20 20:00:52|\n",
      "| 224|2020-03-20 19:57:29| 224|2020-03-20 19:57:29|\n",
      "| 233|2020-03-20 07:40:11| 233|2020-03-20 07:40:11|\n",
      "| 216|2020-03-20 07:40:49| 216|2020-03-20 07:40:49|\n",
      "| 214|2020-03-20 07:37:27| 214|2020-03-20 07:37:27|\n",
      "| 210|2020-03-20 01:09:29| 210|2020-03-20 01:09:29|\n",
      "| 206|2020-03-20 01:30:40| 206|2020-03-20 01:30:40|\n",
      "| 226|2020-03-20 01:46:50| 226|2020-03-20 01:46:50|\n",
      "| 249|2020-03-20 13:29:38| 249|2020-03-20 13:29:38|\n",
      "| 228|2020-03-20 13:28:37| 228|2020-03-20 13:28:37|\n",
      "| 228|2020-03-20 13:28:37| 228|2020-03-20 20:03:41|\n",
      "+----+-------------------+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Directly append new Inserted products\n",
    "from pyspark.sql.functions import *\n",
    "per_day_data_1 = per_day_data.filter(\n",
    "    per_day_data.record_type == 'I').select(['p_id', 'date_timestamp'])\n",
    "\n",
    "ppd_1 = per_day_data_1.alias('ppd_1')\n",
    "ppd_2 = per_day_data_1.alias('ppd_2')\n",
    "\n",
    "per_day_data_2 = ppd_1.join(\n",
    "    ppd_2, ppd_1.p_id == ppd_2.p_id, 'inner').filter(ppd_1.date_timestamp > ppd_2.date_timestamp)\n",
    "\n",
    "per_day_data_2.show()\n",
    "# per_day_data_2 = per_day_data_1.select(\n",
    "#     ['p_id', 'p_name', 'price', 'Date_timestamp'])\n",
    "# mainTable_1 = mainTable.union(per_day_data_2)\n",
    "\n",
    "# # mainTable_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T08:52:50.333473Z",
     "start_time": "2020-03-26T08:52:50.288892Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mainTable_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-78cdcf56aae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Registers this DataFrame as a temporary table using the given name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmainTable_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table_main\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mper_day_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table_day\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mainTable_1' is not defined"
     ]
    }
   ],
   "source": [
    "# Drop deleted products\n",
    "\n",
    "# Registers this DataFrame as a temporary table using the given name\n",
    "mainTable_1.registerTempTable(\"table_main\")\n",
    "per_day_data.registerTempTable(\"table_day\")\n",
    "\n",
    "mainTable_2 = spark.sql(\n",
    "    \"select * from table_main where p_id not in ( select p_id from table_day where record_type = 'D' )\")\n",
    "\n",
    "# mainTable_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-25T19:46:22.670782Z",
     "start_time": "2020-03-25T19:46:21.108652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT p_id)|\n",
      "+--------------------+\n",
      "|                 206|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# update products\n",
    "from pyspark.sql.functions import *\n",
    "mainTable_2.select( countDistinct(mainTable_2.p_id) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-26T08:52:31.783439Z",
     "start_time": "2020-03-26T08:52:31.757764Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mainTable_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5b3c68644f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmainTable_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"p_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"count > 1\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mainTable_2' is not defined"
     ]
    }
   ],
   "source": [
    "mainTable_2.select('p_id').groupBy(\"p_id\").count().filter( \"count > 1\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
