{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest Product Table\n",
    "\n",
    "- p_id from **1 to 100** already present in  main table, it can be updated and deleted\n",
    "- p_id from **101 to 200** can be inserted in to main table and after inserting it into main table it could also be deleted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:54:15.568336Z",
     "start_time": "2020-04-13T11:54:15.539043Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Main entry point for DataFrame and SQL functionality.\n",
    "from pyspark.sql import SparkSession\n",
    "# Start SPARK Session\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:54:21.506313Z",
     "start_time": "2020-04-13T11:54:16.747663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read from main_table\n",
    "Table = spark.read.format('csv').options(\n",
    "    header=True, inferschema=True).load(\n",
    "        \"/home/bluepi/Downloads/Update/product_info/main table/main_table_new.csv\")\n",
    "\n",
    "# Add record type to main table\n",
    "Table_new = Table.withColumn('record_type', lit(\"A\"))\n",
    "\n",
    "# Write in main_table folder\n",
    "path = \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT\"\n",
    "try:\n",
    "#     Table_new.write.format('csv').save(\n",
    "#         os.path.join(path, 'main_table'), header=True)\n",
    "    Table_new.coalesce(1).write.format('csv').save((path), header=True)\n",
    "\n",
    "except:\n",
    "#     Table_new.write.mode('overwrite').format('csv').save(\n",
    "#         os.path.join(path, 'main_table'), header=True)\n",
    "    shutil.rmtree(\"/home/bluepi/Downloads/Update/Updated Product/Latest Product\")\n",
    "    Table_new.coalesce(1).write.format('csv').save((path), header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T11:54:23.519745Z",
     "start_time": "2020-04-13T11:54:23.501226Z"
    }
   },
   "outputs": [],
   "source": [
    "# GFG\n",
    "class my_dictionary(dict):\n",
    "\n",
    "    # __init__ function\n",
    "    def __init__(self):\n",
    "        self = dict()\n",
    "\n",
    "    # Function to add key:value\n",
    "    def add(self, key, value):\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T12:05:12.864509Z",
     "start_time": "2020-04-13T12:05:12.771301Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o332.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-45d020765e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmainTable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_struc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#         path1 = os.path.join(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT;'"
     ]
    }
   ],
   "source": [
    "# Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have inserted,deleted & updated\n",
    "total_products_updated = my_dictionary()\n",
    "total_products_inserted = my_dictionary()\n",
    "total_products_deleted = my_dictionary()\n",
    "\n",
    "# Storing all the p_id in a list which we have deleted and inserted\n",
    "total_products_inserted_list = []\n",
    "total_products_deleted_list = []\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "for i in range(10, 0, -1):\n",
    "\n",
    "    data_schema = [\n",
    "        StructField('p_id', IntegerType(), True),\n",
    "        StructField('p_name', StringType(), True),\n",
    "        StructField('price', IntegerType(), True),\n",
    "        StructField('date_timestamp', TimestampType(), True),\n",
    "        StructField('record_type', StringType(), True)\n",
    "    ]\n",
    "    # Schema which we are accepting\n",
    "    final_struc = StructType(fields=data_schema)\n",
    "\n",
    "    # In this try-catch block we are only taking the previous day data, but initially there is\n",
    "    # no previous day, therefore we are taking main table at begining\n",
    "    try:\n",
    "        #         path1 = os.path.join(\n",
    "        #             \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/\", t)\n",
    "        try:\n",
    "            os.remove(os.path.join(path1, '_SUCCESS'))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        mainTable = spark.read.csv(\n",
    "            \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT\", header=True, schema=final_struc)\n",
    "    except NameError:\n",
    "        #         path1 = os.path.join(\n",
    "        #             \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/\", \"main_table\")\n",
    "        try:\n",
    "            os.remove(os.path.join(path1, '_SUCCESS'))\n",
    "        except:\n",
    "            pass\n",
    "        mainTable = spark.read.csv(\n",
    "            \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT\", header=True, schema=final_struc)\n",
    "\n",
    "#     print(\"\\n File read from --\")\n",
    "#     print(path1, end='\\n')\n",
    "\n",
    "    # Address to the product_info folder\n",
    "    address = \"/home/bluepi/Downloads/Update/product_info/\"\n",
    "    previous_day = (datetime.datetime.today() -\n",
    "                    datetime.timedelta(days=i)).strftime('%d-%m-%Y')\n",
    "    print(\"\\n\\t\\ Date ----> \"+previous_day+\"\\n\")\n",
    "\n",
    "    # before_insert = str(mainTable.count())\n",
    "    print(\"Main_Table Count ----> \" + str(mainTable.count()))\n",
    "\n",
    "    # Address to the Previous Day folder\n",
    "    new_address = address + previous_day\n",
    "#     print(\"\\nNew Address to read the folder ---->\"+new_address)\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "    # Read the Previous Day folder\n",
    "    per_day_data = spark.read.format('csv') \\\n",
    "        .options(header=True, inferschema=True) \\\n",
    "        .load(new_address)\n",
    "\n",
    "    # Get the list of p_id which we have to inserted in the main table\n",
    "    to_be_inserted = per_day_data.filter(\"record_type == 'I' \").collect()\n",
    "    p_id_list_I = [i.p_id for i in to_be_inserted]\n",
    "\n",
    "    # Since we don't want our Inserted product to be Inserted again\n",
    "    for i in total_products_inserted_list:\n",
    "        if i in p_id_list_I:\n",
    "            print(\"\\n{} is going to be inserted again\\n\".format(i))\n",
    "            # Therefore removing already inserted products\n",
    "            p_id_list_I.remove(i)\n",
    "    # Storing all the p_id in a list which we have inserted\n",
    "    total_products_inserted_list.extend(p_id_list_I)\n",
    "\n",
    "    # Directly append new Inserted products\n",
    "    mainTable_I_inserted = mainTable.union(\n",
    "        per_day_data.filter(\"record_type == 'I' \"))\n",
    "\n",
    "    after_insert = str(mainTable_I_inserted.count())\n",
    "    total_insert = str(per_day_data.filter(\"record_type == 'I' \").count())\n",
    "\n",
    "    print(\"Total \\\"Inserted I\\\"----> \" + total_insert)\n",
    "    print(\"After Inserting----> \" + after_insert)\n",
    "\n",
    "    # Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have inserted\n",
    "    total_products_inserted.add(previous_day, p_id_list_I)\n",
    "    p_id_list_I.sort()\n",
    "    print(\" \".join(map(str, p_id_list_I)))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "# Created a new DataFrame of records to be updated\n",
    "    from_per_day_data_U = per_day_data.filter(\"record_type == 'U' \")\n",
    "\n",
    "# Get the list of p_id which we have to update taken from dated folders\n",
    "    from_per_day_data_U_list = from_per_day_data_U.select(\"p_id\").collect()\n",
    "\n",
    "# List comprehension\n",
    "    p_id_list_U = [i.p_id for i in from_per_day_data_U_list]\n",
    "    p_id_list_U.sort()\n",
    "    print(\"TO Be Update\\\"per_day_data\\\"\")\n",
    "    print(\" \".join(map(str, p_id_list_U)))\n",
    "#     print(p_id_list_U)\n",
    "\n",
    "    # Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have updated\n",
    "    total_products_updated.add(previous_day, p_id_list_U)\n",
    "\n",
    "    total_update = str(len((p_id_list_U)))\n",
    "    print(\"Update Count ------>\"+total_update)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Get the products which we have to update, present in Main Table\n",
    "    from_mainTable_U = mainTable_I_inserted.filter(\n",
    "        col('p_id').isin(p_id_list_U))\n",
    "\n",
    "    # Performed Union operation on the above tables\n",
    "    mT_and_pDD_union = from_mainTable_U.union(from_per_day_data_U)\n",
    "#     mT_and_pDD_union.orderBy(mT_and_pDD_union.p_id).show()\n",
    "\n",
    "    # Performed GroupBy operation on P_ID and take the latest date only\n",
    "    #    Rename the columns\n",
    "    for_join_mT_and_pDD = mT_and_pDD_union.groupBy(\"p_id\").agg(\n",
    "        {\"date_timestamp\": \"max\"}).withColumnRenamed(\"max(date_timestamp)\", \"date_timestamp_1\")\n",
    "    for_join_mT_and_pDD = for_join_mT_and_pDD.withColumnRenamed(\n",
    "        \"p_id\", \"p_id_1\")\n",
    "#     for_join_mT_and_pDD.show()\n",
    "\n",
    "    # Performed Join operation to pick only latest updates only\n",
    "    joined = mT_and_pDD_union.join(for_join_mT_and_pDD, (\n",
    "        mT_and_pDD_union.p_id == for_join_mT_and_pDD.p_id_1) & (\n",
    "            mT_and_pDD_union.date_timestamp == for_join_mT_and_pDD.date_timestamp_1), 'inner')\n",
    "\n",
    "    joined = joined.select(\n",
    "        ['p_id', 'p_name', 'price', 'date_timestamp', 'record_type'])\n",
    "#     joined.count()\n",
    "\n",
    "    # First remove the p_id from Main_Table which we have to updated\n",
    "    mainTable_U_updated = mainTable_I_inserted.filter(\n",
    "        ~col('p_id').isin(p_id_list_U))\n",
    "#     mainTable_U_updated.orderBy(\"p_id\").count()\n",
    "\n",
    "    # Then Add the Updated P_ID to the Main_Table\n",
    "    mainTable_U_updated_new = mainTable_U_updated.union(joined)\n",
    "#     after_update = str(mainTable_U_updated_new.orderBy(\"p_id\").count())\n",
    "#     mainTable_U_updated_new.orderBy(\"p_id\").count()\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "    # Get the list of p_id which we have to delete\n",
    "    to_be_deleted = per_day_data.filter(\"record_type == 'D' \").collect()\n",
    "    p_id_list_D = [i.p_id for i in to_be_deleted]\n",
    "\n",
    "    # Since we don't want our deleted product to be deleted again\n",
    "    for i in total_products_deleted_list:\n",
    "        if i in p_id_list_D:\n",
    "            print(\"\\n{} is going to be deleted again\\n\".format(i))\n",
    "            # Therefore removing already deleted products\n",
    "            p_id_list_D.remove(i)\n",
    "    # Storing all the p_id in a list which we have deleted\n",
    "    total_products_deleted_list.extend(p_id_list_D)\n",
    "\n",
    "    # Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have deleted\n",
    "    total_products_deleted.add(previous_day, p_id_list_D)\n",
    "\n",
    "\n",
    "#     print(\"\\nList of p_id which we have to deleted taken from \\\"per_day_data\\\"\")\n",
    "#     print(p_id_list_D)\n",
    "    total_deleted = str(len((p_id_list_D)))\n",
    "    p_id_list_D.sort()\n",
    "    print(\"\\nTotal Deleted ------>\"+str(len((p_id_list_D))))\n",
    "    print(\" \".join(map(str, p_id_list_D)))\n",
    "\n",
    "    # Remove the deleted p_id from main_table\n",
    "    mainTable_D_deleted = mainTable_U_updated_new.filter(\n",
    "        ~col('p_id').isin(p_id_list_D))\n",
    "\n",
    "    after_delete = str(mainTable_D_deleted.count())\n",
    "    print(\"After Deleting Count---->\"+after_delete)\n",
    "\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "    # This writes the DF in different files becaues of parallism\n",
    "    t = \"main_table\"+str(previous_day)\n",
    "#     print(t,end=\"\\n\")\n",
    "\n",
    "#     try:\n",
    "#     mainTable_D_deleted.write.format('csv').save(\n",
    "#             os.path.join(path, t), header=True)\n",
    "#         print(\"\\n Main Table Stored (overwritten-NO)\")\n",
    "#     except:\n",
    "#         mainTable_D_deleted.write.mode('overwrite').format(\n",
    "#             'csv').save(os.path.join(path, t), header=True)\n",
    "#         print(\"\\n Main Table Stored (overwritten-YES)\")\n",
    "\n",
    "    try:\n",
    "        #     Table_new.write.format('csv').save(\n",
    "        #         os.path.join(path, 'main_table'), header=True)\n",
    "        mainTable_D_deleted.write.format('csv').save((path), header=True)\n",
    "\n",
    "    except:\n",
    "        #     Table_new.write.mode('overwrite').format('csv').save(\n",
    "        #         os.path.join(path, 'main_table'), header=True)\n",
    "\n",
    "        shutil.rmtree(\n",
    "            \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/\")\n",
    "        try:\n",
    "            mainTable_D_deleted.write.format('csv').save((path), header=True)\n",
    "        except:\n",
    "            os.chmod(\n",
    "                \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT\", 0o777)\n",
    "            os.rmdir(\n",
    "                \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/MT\")\n",
    "#             mainTable_D_deleted.write.format('csv').save((path,), header=True)\n",
    "            mainTable_D_deleted.write.csv(path, mode=overwrite, header=True)\n",
    "\n",
    "    print(\"\\n Main Table Stored \")\n",
    "    print(os.path.join(path, t))\n",
    "\n",
    "    print(\"\\n***************************************************************************************\\n\")\n",
    "#     input(\"Press Enter to continue...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products Updated in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:02:57.560156Z",
     "start_time": "2020-04-10T14:02:52.908Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l = mT_and_pDD_union.groupBy(\"p_id\").agg({'p_id':'count'}).filter(\"count(p_id)>1\").select('p_id').collect()\n",
    "l1 = [ i.p_id for i in l]\n",
    "mT_and_pDD_union.filter( col('p_id').isin(l1) ).orderBy(col('p_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:02:57.561880Z",
     "start_time": "2020-04-10T14:02:52.912Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mainTable_D_deleted.filter( col('p_id').isin(l1) ).orderBy(col('p_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Insertion, Updation & Deletion Day Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:02:57.563195Z",
     "start_time": "2020-04-10T14:02:52.915Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "# prints the formatted representation of PrettyPrinter object\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for i in range(10,0,-1) :\n",
    "    previous_day = (datetime.datetime.today() - datetime.timedelta(days=i)).strftime('%d-%m-%Y')\n",
    "    pp.pprint(previous_day)\n",
    "    print(\"inserted :\\t\")\n",
    "    print(' '.join(map(str, total_products_inserted.get(previous_day))))\n",
    "#     pp.pprint(total_products_inserted.get(previous_day))\n",
    "    print(\"updated :\\t\")\n",
    "    print(' '.join(map(str, total_products_updated.get(previous_day))))\n",
    "#     pp.pprint(total_products_updated.get(previous_day))\n",
    "    print(\"deleted :\\t\")\n",
    "    print(' '.join(map(str, total_products_deleted.get(previous_day))))\n",
    "#     pp.pprint(total_products_deleted.get(previous_day))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Deleted and Inserted Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:02:57.564552Z",
     "start_time": "2020-04-10T14:02:52.918Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_products_deleted_list.sort()\n",
    "print(' '.join(map(str,total_products_deleted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T14:02:57.566176Z",
     "start_time": "2020-04-10T14:02:52.921Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_products_inserted_list.sort()\n",
    "print(' '.join(map(str,total_products_inserted_list)))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
