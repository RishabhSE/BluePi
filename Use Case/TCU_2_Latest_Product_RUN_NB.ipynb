{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest Product Table\n",
    "\n",
    "- p_id from **1 to 100** already present in  main table, it can be updated and deleted\n",
    "- p_id from **101 to 200** can be inserted in to main table and after inserting it into main table it could also be deleted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:44:03.016640Z",
     "start_time": "2020-04-15T09:44:03.002877Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "# Main entry point for DataFrame and SQL functionality.\n",
    "from pyspark.sql import SparkSession\n",
    "# Start SPARK Session\n",
    "spark = SparkSession.builder.appName('Basics').getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:44:08.204108Z",
     "start_time": "2020-04-15T09:44:03.018654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read from main_table\n",
    "Table = spark.read.format('csv').options(\n",
    "    header=True, inferschema=True).load(\n",
    "        \"/home/bluepi/Downloads/Update/product_info/main table/main_table_new.csv\")\n",
    "\n",
    "# Add record type to main table\n",
    "Table_new = Table.withColumn('record_type', lit(\"A\"))\n",
    "\n",
    "# Write in main_table folder\n",
    "path = \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/\"\n",
    "try:\n",
    "    Table_new.write.format('csv').save(\n",
    "        os.path.join(path, 'main_table'), header=True)\n",
    "except:\n",
    "    Table_new.write.mode('overwrite').format('csv').save(\n",
    "        os.path.join(path, 'main_table'), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:44:08.215009Z",
     "start_time": "2020-04-15T09:44:08.210259Z"
    }
   },
   "outputs": [],
   "source": [
    "# GFG\n",
    "class my_dictionary(dict):\n",
    "\n",
    "    # __init__ function\n",
    "    def __init__(self):\n",
    "        self = dict()\n",
    "\n",
    "    # Function to add key:value\n",
    "    def add(self, key, value):\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:45:13.567860Z",
     "start_time": "2020-04-15T09:44:08.221232Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table\n",
      "\n",
      "\t\\ Date ----> 05-04-2020\n",
      "\n",
      "Main_Table Count ----> 100\n",
      "Total \"Inserted I\"----> 9\n",
      "After Inserting----> 109\n",
      "108 112 116 118 134 147 155 173 185\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "2 5 6 8 14 15 25 36 39 41 44 57 59 60 63 63 68 71 71 81 82 83 84 85 86 92 95 95 97 134 185\n",
      "Update Count ------>31\n",
      "\n",
      "\n",
      "\n",
      "Total Deleted ------>10\n",
      "9 20 30 40 47 61 63 70 72 100\n",
      "After Deleting Count---->99\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table05-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table05-04-2020\n",
      "\n",
      "\t\\ Date ----> 06-04-2020\n",
      "\n",
      "Main_Table Count ----> 99\n",
      "Total \"Inserted I\"----> 10\n",
      "After Inserting----> 109\n",
      "105 109 123 139 166 167 175 176 178 186\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "7 15 15 31 51 55 56 57 64 75 78 83 92 96\n",
      "Update Count ------>14\n",
      "\n",
      "\n",
      "\n",
      "Total Deleted ------>5\n",
      "3 11 94 98 116\n",
      "After Deleting Count---->104\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table06-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table06-04-2020\n",
      "\n",
      "\t\\ Date ----> 07-04-2020\n",
      "\n",
      "Main_Table Count ----> 104\n",
      "Total \"Inserted I\"----> 13\n",
      "After Inserting----> 117\n",
      "101 102 110 117 128 144 157 162 179 180 182 188 192\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "2 3 8 16 28 32 41 49 53 56 71 76 86 90 92 105 108 162 166 167\n",
      "Update Count ------>20\n",
      "\n",
      "\n",
      "\n",
      "Total Deleted ------>7\n",
      "13 18 23 56 62 68 180\n",
      "After Deleting Count---->111\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table07-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table07-04-2020\n",
      "\n",
      "\t\\ Date ----> 08-04-2020\n",
      "\n",
      "Main_Table Count ----> 111\n",
      "Total \"Inserted I\"----> 9\n",
      "After Inserting----> 120\n",
      "104 111 120 121 141 148 152 170 171\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "6 7 15 16 17 29 35 42 46 48 53 55 71 74 85 95 141 157 176 179 182\n",
      "Update Count ------>21\n",
      "\n",
      "\n",
      "\n",
      "Total Deleted ------>1\n",
      "69\n",
      "After Deleting Count---->119\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table08-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table08-04-2020\n",
      "\n",
      "\t\\ Date ----> 09-04-2020\n",
      "\n",
      "Main_Table Count ----> 119\n",
      "Total \"Inserted I\"----> 11\n",
      "After Inserting----> 130\n",
      "122 129 130 133 150 156 159 174 174 189 197\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "3 7 37 53 54 54 57 60 68 71 75 76 77 82 82 85 95 101 175 182 185 185\n",
      "Update Count ------>22\n",
      "\n",
      "\n",
      "\n",
      "Total Deleted ------>5\n",
      "2 78 81 86 173\n",
      "After Deleting Count---->126\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table09-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table09-04-2020\n",
      "\n",
      "\t\\ Date ----> 10-04-2020\n",
      "\n",
      "Main_Table Count ----> 126\n",
      "Total \"Inserted I\"----> 11\n",
      "After Inserting----> 137\n",
      "106 107 113 114 125 140 142 149 177 194 198\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "22 32 33 39 45 48 51 51 74 80 84 88 92 97 101 102 123 139 139 141 147 175 177 178 194\n",
      "Update Count ------>25\n",
      "\n",
      "\n",
      "\n",
      "68 is going to be deleted again\n",
      "\n",
      "\n",
      "Total Deleted ------>4\n",
      "45 67 108 129\n",
      "After Deleting Count---->133\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table10-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table10-04-2020\n",
      "\n",
      "\t\\ Date ----> 11-04-2020\n",
      "\n",
      "Main_Table Count ----> 133\n",
      "Total \"Inserted I\"----> 10\n",
      "After Inserting----> 143\n",
      "103 124 137 143 143 154 160 165 169 183\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "3 10 12 22 27 48 50 80 82 110 134 143 152 155 177 179 186 197\n",
      "Update Count ------>18\n",
      "\n",
      "\n",
      "\n",
      "56 is going to be deleted again\n",
      "\n",
      "\n",
      "Total Deleted ------>7\n",
      "12 31 44 56 79 114 150\n",
      "After Deleting Count---->136\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table11-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table11-04-2020\n",
      "\n",
      "\t\\ Date ----> 12-04-2020\n",
      "\n",
      "Main_Table Count ----> 136\n",
      "Total \"Inserted I\"----> 7\n",
      "After Inserting----> 143\n",
      "119 127 136 146 151 163 172\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "8 35 49 65 103 103 104 107 107 107 110 121 127 139 141 154 166 174 174 178 188 194\n",
      "Update Count ------>22\n",
      "\n",
      "\n",
      "\n",
      "Total Deleted ------>8\n",
      "16 26 38 99 125 147 185 198\n",
      "After Deleting Count---->134\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table12-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table12-04-2020\n",
      "\n",
      "\t\\ Date ----> 13-04-2020\n",
      "\n",
      "Main_Table Count ----> 134\n",
      "Total \"Inserted I\"----> 6\n",
      "After Inserting----> 140\n",
      "145 153 187 193 196 200\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "4 6 15 39 52 53 54 66 80 85 90 90 92 97 102 107 110 121 177 188 193\n",
      "Update Count ------>21\n",
      "\n",
      "\n",
      "\n",
      "81 is going to be deleted again\n",
      "\n",
      "\n",
      "Total Deleted ------>2\n",
      "134 144\n",
      "After Deleting Count---->138\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table13-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n",
      "\n",
      " File read from --\n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table13-04-2020\n",
      "\n",
      "\t\\ Date ----> 14-04-2020\n",
      "\n",
      "Main_Table Count ----> 138\n",
      "Total \"Inserted I\"----> 8\n",
      "After Inserting----> 146\n",
      "115 126 161 168 184 190 191 199\n",
      "\n",
      "\n",
      "TO Be Update\"per_day_data\"\n",
      "8 39 42 66 71 88 152 155 157\n",
      "Update Count ------>9\n",
      "\n",
      "\n",
      "\n",
      "Total Deleted ------>6\n",
      "37 77 89 154 160 186\n",
      "After Deleting Count---->140\n",
      "\n",
      " Main Table Stored \n",
      "/home/bluepi/Downloads/Update/Updated Product/Latest Product/main_table14-04-2020\n",
      "\n",
      "***************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have inserted,deleted & updated            \n",
    "total_products_updated = my_dictionary()\n",
    "total_products_inserted = my_dictionary()\n",
    "total_products_deleted = my_dictionary()\n",
    "\n",
    "# Storing all the p_id in a list which we have deleted and inserted\n",
    "total_products_inserted_list = []\n",
    "total_products_deleted_list = []\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "for i in range(10, 0, -1):\n",
    "\n",
    "    data_schema = [\n",
    "        StructField('p_id', IntegerType(), True),\n",
    "        StructField('p_name', StringType(), True),\n",
    "        StructField('price', IntegerType(), True),\n",
    "        StructField('date_timestamp', TimestampType(), True),\n",
    "        StructField('record_type', StringType(), True)\n",
    "    ]\n",
    "    # Schema which we are accepting\n",
    "    final_struc = StructType(fields=data_schema)\n",
    "\n",
    "    # In this try-catch block we are only taking the previous day data, but initially there is\n",
    "    # no previous day, therefore we are taking main table at begining\n",
    "    try:\n",
    "        path1 = os.path.join(\n",
    "            \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/\", t)\n",
    "        try:\n",
    "            os.remove(os.path.join(path1, '_SUCCESS'))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        mainTable_old = spark.read.csv(path1, header=True, schema=final_struc)\n",
    "        mainTable = spark.createDataFrame([], schema=final_struc)\n",
    "        mainTable = mainTable.union(mainTable_old)\n",
    "    except:\n",
    "        path1 = os.path.join(\n",
    "            \"/home/bluepi/Downloads/Update/Updated Product/Latest Product/\", \"main_table\")\n",
    "        try:\n",
    "            os.remove(os.path.join(path1, '_SUCCESS'))\n",
    "        except:\n",
    "            pass\n",
    "        mainTable_old = spark.read.csv(path1, header=True, schema=final_struc)\n",
    "        mainTable = spark.createDataFrame([], schema=final_struc)\n",
    "        mainTable = mainTable.union(mainTable_old)\n",
    "\n",
    "    print(\"\\n File read from --\")\n",
    "    print(path1, end='\\n')\n",
    "\n",
    "    # Address to the product_info folder\n",
    "    address = \"/home/bluepi/Downloads/Update/product_info/\"\n",
    "    previous_day = (datetime.datetime.today() -\n",
    "                    datetime.timedelta(days=i)).strftime('%d-%m-%Y')\n",
    "    print(\"\\n\\t\\ Date ----> \"+previous_day+\"\\n\")\n",
    "\n",
    "    # before_insert = str(mainTable.count())\n",
    "    print(\"Main_Table Count ----> \" + str(mainTable.count()))\n",
    "\n",
    "    # Address to the Previous Day folder\n",
    "    new_address = address + previous_day\n",
    "#     print(\"\\nNew Address to read the folder ---->\"+new_address)\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "    # Read the Previous Day folder\n",
    "    per_day_data = spark.read.format('csv') \\\n",
    "        .options(header=True, inferschema=True) \\\n",
    "        .load(new_address)\n",
    "\n",
    "    # Get the list of p_id which we have to inserted in the main table\n",
    "    to_be_inserted = per_day_data.filter(\"record_type == 'I' \").collect()\n",
    "    p_id_list_I = [i.p_id for i in to_be_inserted]\n",
    "    \n",
    "    # Since we don't want our Inserted product to be Inserted again\n",
    "    for i in total_products_inserted_list :\n",
    "        if i in p_id_list_I :\n",
    "            print(\"\\n{} is going to be inserted again\\n\".format(i))\n",
    "            # Therefore removing already inserted products\n",
    "            p_id_list_I.remove(i)\n",
    "    # Storing all the p_id in a list which we have inserted\n",
    "    total_products_inserted_list.extend(p_id_list_I)\n",
    "\n",
    "    # Directly append new Inserted products\n",
    "    mainTable_I_inserted = mainTable.union(\n",
    "        per_day_data.filter(\"record_type == 'I' \"))\n",
    "\n",
    "    after_insert = str(mainTable_I_inserted.count())\n",
    "    total_insert = str(per_day_data.filter(\"record_type == 'I' \").count())\n",
    "\n",
    "    print(\"Total \\\"Inserted I\\\"----> \" + total_insert)\n",
    "    print(\"After Inserting----> \" + after_insert)\n",
    "\n",
    "    # Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have inserted\n",
    "    total_products_inserted.add(previous_day, p_id_list_I)\n",
    "    p_id_list_I.sort()\n",
    "    print(\" \".join(map(str, p_id_list_I)))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "# Created a new DataFrame of records to be updated\n",
    "    from_per_day_data_U = per_day_data.filter(\"record_type == 'U' \")\n",
    "    \n",
    "# Get the list of p_id which we have to update taken from dated folders\n",
    "    from_per_day_data_U_list = from_per_day_data_U.select(\"p_id\").collect()\n",
    "\n",
    "# List comprehension\n",
    "    p_id_list_U = [i.p_id for i in from_per_day_data_U_list]\n",
    "    p_id_list_U.sort()\n",
    "    print(\"TO Be Update\\\"per_day_data\\\"\")\n",
    "    print(\" \".join(map(str, p_id_list_U)))\n",
    "#     print(p_id_list_U)\n",
    "\n",
    "    # Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have updated\n",
    "    total_products_updated.add(previous_day, p_id_list_U)\n",
    "\n",
    "    total_update = str(len((p_id_list_U)))\n",
    "    print(\"Update Count ------>\"+total_update)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Get the products which we have to update, present in Main Table\n",
    "    from_mainTable_U = mainTable_I_inserted.filter(\n",
    "        col('p_id').isin(p_id_list_U))\n",
    "\n",
    "    # Performed Union operation on the above tables\n",
    "    mT_and_pDD_union = from_mainTable_U.union(from_per_day_data_U)\n",
    "#     mT_and_pDD_union.orderBy(mT_and_pDD_union.p_id).show()\n",
    "\n",
    "    # Performed GroupBy operation on P_ID and take the latest date only\n",
    "    #    Rename the columns\n",
    "    for_join_mT_and_pDD = mT_and_pDD_union.groupBy(\"p_id\").agg(\n",
    "        {\"date_timestamp\": \"max\"}).withColumnRenamed(\"max(date_timestamp)\", \"date_timestamp_1\")\n",
    "    for_join_mT_and_pDD = for_join_mT_and_pDD.withColumnRenamed(\n",
    "        \"p_id\", \"p_id_1\")\n",
    "#     for_join_mT_and_pDD.show()\n",
    "\n",
    "    # Performed Join operation to pick only latest updates only\n",
    "    joined = mT_and_pDD_union.join(for_join_mT_and_pDD, (\n",
    "        mT_and_pDD_union.p_id == for_join_mT_and_pDD.p_id_1) & (\n",
    "            mT_and_pDD_union.date_timestamp == for_join_mT_and_pDD.date_timestamp_1), 'inner')\n",
    "\n",
    "    joined = joined.select(\n",
    "        ['p_id', 'p_name', 'price', 'date_timestamp', 'record_type'])\n",
    "#     joined.count()\n",
    "\n",
    "    # First remove the p_id from Main_Table which we have to updated\n",
    "    mainTable_U_updated = mainTable_I_inserted.filter(\n",
    "        ~col('p_id').isin(p_id_list_U))\n",
    "#     mainTable_U_updated.orderBy(\"p_id\").count()\n",
    "\n",
    "    # Then Add the Updated P_ID to the Main_Table\n",
    "    mainTable_U_updated_new = mainTable_U_updated.union(joined)\n",
    "#     after_update = str(mainTable_U_updated_new.orderBy(\"p_id\").count())\n",
    "#     mainTable_U_updated_new.orderBy(\"p_id\").count()\n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "    # Get the list of p_id which we have to delete\n",
    "    to_be_deleted = per_day_data.filter(\"record_type == 'D' \").collect()\n",
    "    p_id_list_D = [i.p_id for i in to_be_deleted]\n",
    "    \n",
    "    # Since we don't want our deleted product to be deleted again\n",
    "    for i in total_products_deleted_list :\n",
    "        if i in p_id_list_D :\n",
    "            print(\"\\n{} is going to be deleted again\\n\".format(i))\n",
    "            # Therefore removing already deleted products\n",
    "            p_id_list_D.remove(i)\n",
    "    # Storing all the p_id in a list which we have deleted            \n",
    "    total_products_deleted_list.extend(p_id_list_D)\n",
    "            \n",
    "    \n",
    "    # Storing all the \"p_id as value\" in a dictionary with \"date as key\", which we have deleted\n",
    "    total_products_deleted.add(previous_day, p_id_list_D)\n",
    "    \n",
    "\n",
    "#     print(\"\\nList of p_id which we have to deleted taken from \\\"per_day_data\\\"\")\n",
    "#     print(p_id_list_D)\n",
    "    total_deleted = str(len((p_id_list_D)))\n",
    "    p_id_list_D.sort()\n",
    "    print(\"\\nTotal Deleted ------>\"+str(len((p_id_list_D))))\n",
    "    print(\" \".join(map(str, p_id_list_D)))\n",
    "\n",
    "    # Remove the deleted p_id from main_table\n",
    "    mainTable_D_deleted = mainTable_U_updated_new.filter(\n",
    "        ~col('p_id').isin(p_id_list_D))\n",
    "\n",
    "    after_delete = str(mainTable_D_deleted.count())\n",
    "    print(\"After Deleting Count---->\"+after_delete)\n",
    "    \n",
    "\n",
    "# ###################################################################################################\n",
    "\n",
    "    # This writes the DF in different files becaues of parallism\n",
    "    t = \"main_table\"+str(previous_day)\n",
    "#     print(t,end=\"\\n\")\n",
    "\n",
    "\n",
    "    mainTable_D_deleted.write.format('csv').save(\n",
    "            os.path.join(path, t), header=True)\n",
    "\n",
    "# Since, coalesce opertation might become costly when working with large files, therefore not using it.\n",
    "#     mainTable_D_deleted.coalesce(1).write.format('csv').save(\n",
    "#             os.path.join(path, t), header=True)\n",
    "\n",
    "\n",
    "    print(\"\\n Main Table Stored \")\n",
    "    print(os.path.join(path, t))\n",
    "\n",
    "    print(\"\\n***************************************************************************************\\n\")\n",
    "#     input(\"Press Enter to continue...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Products Updated in a day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:45:14.872015Z",
     "start_time": "2020-04-15T09:45:13.570157Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+-------------------+-----------+\n",
      "|p_id|   p_name|price|     date_timestamp|record_type|\n",
      "+----+---------+-----+-------------------+-----------+\n",
      "|   8| Alphazap| 1044|2020-04-11 15:29:28|          U|\n",
      "|   8| Alphazap|  672|2020-04-13 03:16:18|          U|\n",
      "|  39|   Sonair|  186|2020-04-13 03:27:38|          U|\n",
      "|  39|   Sonair| 1078|2020-04-12 21:25:17|          U|\n",
      "|  42|  Andalax| 1276|2020-04-07 09:26:57|          U|\n",
      "|  42|  Andalax|  889|2020-04-13 03:43:08|          U|\n",
      "|  66|Gembucket|  611|2020-04-13 21:20:37|          U|\n",
      "|  66|Gembucket|  764|2020-04-12 21:45:42|          U|\n",
      "|  71|   Keylex|  523|2020-04-08 21:44:27|          U|\n",
      "|  71|   Keylex| 1131|2020-04-13 03:15:40|          U|\n",
      "|  88|  Redhold|  748|2020-04-09 15:42:24|          U|\n",
      "|  88|  Redhold|  721|2020-04-13 03:39:50|          U|\n",
      "| 152|  Prodder|  110|2020-04-10 15:14:29|          U|\n",
      "| 152|  Prodder|  299|2020-04-13 09:45:37|          U|\n",
      "| 155| Bytecard|  239|2020-04-13 21:13:14|          U|\n",
      "| 155| Bytecard|  454|2020-04-10 03:22:05|          U|\n",
      "| 157|   Lotlux|  828|2020-04-07 21:58:47|          U|\n",
      "| 157|   Lotlux| 1191|2020-04-13 15:43:26|          U|\n",
      "+----+---------+-----+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l = mT_and_pDD_union.groupBy(\"p_id\").agg({'p_id':'count'}).filter(\"count(p_id)>1\").select('p_id').collect()\n",
    "l1 = [ i.p_id for i in l]\n",
    "mT_and_pDD_union.filter( col('p_id').isin(l1) ).orderBy(col('p_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Main Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:45:16.818457Z",
     "start_time": "2020-04-15T09:45:14.874219Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----+-------------------+-----------+\n",
      "|p_id|   p_name|price|     date_timestamp|record_type|\n",
      "+----+---------+-----+-------------------+-----------+\n",
      "|   8| Alphazap|  672|2020-04-13 03:16:18|          U|\n",
      "|  39|   Sonair|  186|2020-04-13 03:27:38|          U|\n",
      "|  42|  Andalax|  889|2020-04-13 03:43:08|          U|\n",
      "|  66|Gembucket|  611|2020-04-13 21:20:37|          U|\n",
      "|  71|   Keylex| 1131|2020-04-13 03:15:40|          U|\n",
      "|  88|  Redhold|  721|2020-04-13 03:39:50|          U|\n",
      "| 152|  Prodder|  299|2020-04-13 09:45:37|          U|\n",
      "| 155| Bytecard|  239|2020-04-13 21:13:14|          U|\n",
      "| 157|   Lotlux| 1191|2020-04-13 15:43:26|          U|\n",
      "+----+---------+-----+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainTable_D_deleted.filter( col('p_id').isin(l1) ).orderBy(col('p_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Insertion, Updation & Deletion Day Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:45:16.852364Z",
     "start_time": "2020-04-15T09:45:16.822259Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'05-04-2020'\n",
      "inserted :\t\n",
      "108 112 116 118 134 147 155 173 185\n",
      "updated :\t\n",
      "2 5 6 8 14 15 25 36 39 41 44 57 59 60 63 63 68 71 71 81 82 83 84 85 86 92 95 95 97 134 185\n",
      "deleted :\t\n",
      "9 20 30 40 47 61 63 70 72 100\n",
      "\n",
      "\n",
      "'06-04-2020'\n",
      "inserted :\t\n",
      "105 109 123 139 166 167 175 176 178 186\n",
      "updated :\t\n",
      "7 15 15 31 51 55 56 57 64 75 78 83 92 96\n",
      "deleted :\t\n",
      "3 11 94 98 116\n",
      "\n",
      "\n",
      "'07-04-2020'\n",
      "inserted :\t\n",
      "101 102 110 117 128 144 157 162 179 180 182 188 192\n",
      "updated :\t\n",
      "2 3 8 16 28 32 41 49 53 56 71 76 86 90 92 105 108 162 166 167\n",
      "deleted :\t\n",
      "13 18 23 56 62 68 180\n",
      "\n",
      "\n",
      "'08-04-2020'\n",
      "inserted :\t\n",
      "104 111 120 121 141 148 152 170 171\n",
      "updated :\t\n",
      "6 7 15 16 17 29 35 42 46 48 53 55 71 74 85 95 141 157 176 179 182\n",
      "deleted :\t\n",
      "69\n",
      "\n",
      "\n",
      "'09-04-2020'\n",
      "inserted :\t\n",
      "122 129 130 133 150 156 159 174 174 189 197\n",
      "updated :\t\n",
      "3 7 37 53 54 54 57 60 68 71 75 76 77 82 82 85 95 101 175 182 185 185\n",
      "deleted :\t\n",
      "2 78 81 86 173\n",
      "\n",
      "\n",
      "'10-04-2020'\n",
      "inserted :\t\n",
      "106 107 113 114 125 140 142 149 177 194 198\n",
      "updated :\t\n",
      "22 32 33 39 45 48 51 51 74 80 84 88 92 97 101 102 123 139 139 141 147 175 177 178 194\n",
      "deleted :\t\n",
      "45 67 108 129\n",
      "\n",
      "\n",
      "'11-04-2020'\n",
      "inserted :\t\n",
      "103 124 137 143 143 154 160 165 169 183\n",
      "updated :\t\n",
      "3 10 12 22 27 48 50 80 82 110 134 143 152 155 177 179 186 197\n",
      "deleted :\t\n",
      "12 31 44 56 79 114 150\n",
      "\n",
      "\n",
      "'12-04-2020'\n",
      "inserted :\t\n",
      "119 127 136 146 151 163 172\n",
      "updated :\t\n",
      "8 35 49 65 103 103 104 107 107 107 110 121 127 139 141 154 166 174 174 178 188 194\n",
      "deleted :\t\n",
      "16 26 38 99 125 147 185 198\n",
      "\n",
      "\n",
      "'13-04-2020'\n",
      "inserted :\t\n",
      "145 153 187 193 196 200\n",
      "updated :\t\n",
      "4 6 15 39 52 53 54 66 80 85 90 90 92 97 102 107 110 121 177 188 193\n",
      "deleted :\t\n",
      "134 144\n",
      "\n",
      "\n",
      "'14-04-2020'\n",
      "inserted :\t\n",
      "115 126 161 168 184 190 191 199\n",
      "updated :\t\n",
      "8 39 42 66 71 88 152 155 157\n",
      "deleted :\t\n",
      "37 77 89 154 160 186\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# prints the formatted representation of PrettyPrinter object\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for i in range(10,0,-1) :\n",
    "    previous_day = (datetime.datetime.today() - datetime.timedelta(days=i)).strftime('%d-%m-%Y')\n",
    "    pp.pprint(previous_day)\n",
    "    print(\"inserted :\\t\")\n",
    "    print(' '.join(map(str, total_products_inserted.get(previous_day))))\n",
    "#     pp.pprint(total_products_inserted.get(previous_day))\n",
    "    print(\"updated :\\t\")\n",
    "    print(' '.join(map(str, total_products_updated.get(previous_day))))\n",
    "#     pp.pprint(total_products_updated.get(previous_day))\n",
    "    print(\"deleted :\\t\")\n",
    "    print(' '.join(map(str, total_products_deleted.get(previous_day))))\n",
    "#     pp.pprint(total_products_deleted.get(previous_day))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Deleted and Inserted Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:45:16.863409Z",
     "start_time": "2020-04-15T09:45:16.856779Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 9 11 12 13 16 18 20 23 26 30 31 37 38 40 44 45 47 56 56 61 62 63 67 68 69 70 72 77 78 79 81 86 89 94 98 99 100 108 114 116 125 129 134 144 147 150 154 160 173 180 185 186 198\n"
     ]
    }
   ],
   "source": [
    "total_products_deleted_list.sort()\n",
    "print(' '.join(map(str,total_products_deleted_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-15T09:45:16.876321Z",
     "start_time": "2020-04-15T09:45:16.868391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 133 134 136 137 139 140 141 142 143 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 159 160 161 162 163 165 166 167 168 169 170 171 172 173 174 174 175 176 177 178 179 180 182 183 184 185 186 187 188 189 190 191 192 193 194 196 197 198 199 200\n"
     ]
    }
   ],
   "source": [
    "total_products_inserted_list.sort()\n",
    "print(' '.join(map(str,total_products_inserted_list)))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
